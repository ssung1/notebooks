{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we used one input.  We call that a feature, or $x$.  And we also introduced the dummy feature $x_0$ that is always 1.  This completes our equation for the hypothesis, $h_\\theta$, which is based on all the features plus our parameters, $\\theta$:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sum_{j=1}^n(\\theta_jx_j)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\theta_j$ is our parameter at index $j$, which is our goal in linear regression\n",
    "- $x_j$ is our input feature at index $j$\n",
    "- $n$ is the number of features\n",
    "- $j$ is the index of the feature\n",
    "\n",
    "For example, when we have one feature, we would end up with this equation that looks like a basic linear equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1\n",
    "$$\n",
    "\n",
    "(remember $x_0$ is 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have multiple samples, for each sample, we will indicate the hypothesis of the sample as $h_\\theta(x)^{(i)}$, with a little $i$ on top, meaning the $i$th sample.  And our $i$th training $y$ would be indicated by $y^{(i)}$.\n",
    "\n",
    "The steps of linear regression are\n",
    "\n",
    "1. Pick some $\\theta_{0}$ and $\\theta_{1}$\n",
    "2. Calculate $h_{\\theta}(x)$ for every given sample of $x$\n",
    "3. Compare the calculated value with the actual value.  Calculate the overall error level (known as cost function $J$)\n",
    "   by using\n",
    "   $$\n",
    "     \\frac12\\sum_{i=1}^m(h_\\theta(x)^{(i)} - y^{(i)})^2\n",
    "   $$\n",
    "   where    \n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "4. Minimize the cost function $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the official definition but hard to comprehend all at once.\n",
    "- To make our lives easier, collectively we will call $\\theta_0$, $\\theta_1$, $\\theta_2$, ... as $\\theta$.  As we will see later, linear algebra allows us to treat arrays as single values.\n",
    "- Similarly, $x$ is all our input values (every feature and every sample)\n",
    "- $y$ is all our training target\n",
    "- $h_\\theta(x)$ is our hypothesis calcuated with $x$ and $\\theta$\n",
    "\n",
    "Now our simplified cost $J$ would look like:\n",
    "\n",
    "$$\n",
    "\\frac12(h_\\theta(x) - y)^2\n",
    "$$\n",
    "\n",
    "Kind of makes sense, without all the $\\sum$ and tiny symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will try a \"true\" linear training set.\n",
    "\n",
    "|x  |y  |\n",
    "|---|---|\n",
    "|1  |3  |\n",
    "|5  |11 |\n",
    "|2  |5  |\n",
    "\n",
    "At this point, we are still using one input feature so we are not overwhelmed when we try to debug our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = np.array([1, 5, 2])\n",
    "training_y = np.array([3, 11, 5])\n",
    "theta = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we just added the $x_0$ into our initial training data.  There was only one sample, so it was easy.  But with multiple samples, training data $x$ looks like \n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\\\\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "We want to pad it with a 1 so that the number of input variables (in our case, 1) equals the number of thetas (2).  The number of thetas is always 1 more than the number of input variables.  Don't get confused -- we have only one input variable for every training sample; however, we do have multiple training samples.  Refer to our notation guide from the previous example:\n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "\n",
    "In this example, $n$ is 1.\n",
    "\n",
    "Once we pad the 1, we will have this training data:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\ \\ \\ 1 \\\\\n",
    "1 \\ \\ \\ 5 \\\\\n",
    "1 \\ \\ \\ 2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Our hypothesis is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1\\theta_0 + 1\\theta_1 \\\\\n",
    "1\\theta_0 + 5\\theta_1 \\\\\n",
    "1\\theta_0 + 2\\theta_1 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 5.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want\n",
    "#    theta0 * x0 + theta1 * x1\n",
    "# (and remember x0 is set to 1 to simplify calculation)\n",
    "def padded_training_x(training_x):\n",
    "    # training_x looks like 1, 5, 2\n",
    "    size_of_sample = training_x.size\n",
    "    padding = np.ones(size_of_sample)    \n",
    "    padded = np.array([padding, training_x])\n",
    "    # but we want it in a different orientation\n",
    "    return padded.T\n",
    "padded_training_x(training_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hypothesis_theta(training_x, theta):\n",
    "    return padded_training_x(training_x) @ theta\n",
    "current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "current_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hypothesis for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost(hypothesis, training_y):\n",
    "    return np.sum((hypothesis - training_y) ** 2) / 2\n",
    "current_cost = cost(current_hypothesis, training_y)\n",
    "current_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still one cost because cost is based on all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calclate our slope.  For completeness, here's the official equation:\n",
    "\n",
    "$$\n",
    "slope = \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19., -68.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slope(hypothesis, training_y, training_x):\n",
    "    # we have our padded training samples\n",
    "    # 1  1\n",
    "    # 1  5\n",
    "    # 1  2\n",
    "\n",
    "    # for each row, we want to multiply by the difference\n",
    "    # 1 x -3    1 x -3\n",
    "    # 1 x -11   5 x -11\n",
    "    # 1 x -15   2 x -15\n",
    "\n",
    "    # then we want to sum up by column\n",
    "    # 1 x -3 + 1 x - 11 + 1 x -15         1 x -3 + 5 x -11 + 2 x -15\n",
    "    \n",
    "    # looks awefully like\n",
    "    #\n",
    "    # | 1 1 1 |     | -3|\n",
    "    # | 1 5 2 |  X  |-11|\n",
    "    #               |-15|\n",
    "    return padded_training_x(training_x).T @ (hypothesis - training_y)\n",
    "current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "current_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the same equation as in previous examples but with multiple features and samples.  Suddenly it looks impossible to compute, or does it? (Remember, there is one slope for each feature.)\n",
    "\n",
    "And our gradient descent step becomes:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9, 6.8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(theta, hypothesis, training_y, learning_rate_alpha):\n",
    "    return theta - learning_rate_alpha * slope(hypothesis, training_y, training_x)\n",
    "gradient_descent(theta, current_hypothesis, training_y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum(training_x, training_y):\n",
    "    max_iterations = 100\n",
    "    theta = np.array([0, 0])\n",
    "    iteration = 0\n",
    "    acceptable_slope = 0.00005\n",
    "    learning_rate_alpha = 0.01\n",
    "    current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "    current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "    current_cost = cost(hypothesis_theta(training_x, theta), training_y)\n",
    "    while abs(np.average(current_slope)) > acceptable_slope and iteration < max_iterations:\n",
    "        current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "        current_cost = cost(current_hypothesis, training_y)\n",
    "        current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "        theta = gradient_descent(theta, current_hypothesis, training_y, learning_rate_alpha)\n",
    "        print((theta, current_hypothesis, current_slope, current_cost))\n",
    "        iteration += 1\n",
    "    return (theta, current_hypothesis, current_slope, current_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through one round of gradient descent just to make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_round_0 = np.array([0, 0])\n",
    "#          x   hypothesis\n",
    "# (0)(1) + 0            0\n",
    "# (0)(5) + 0            0\n",
    "# (0)(2) + 0            0\n",
    "hypothesis_round_0 = hypothesis_theta(training_x, theta_round_0)\n",
    "hypothesis_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  difference**2\n",
    "# (0)(1) + 0            0  3          -3              9\n",
    "# (0)(5) + 0            0 11         -11            121\n",
    "# (0)(2) + 0            0  5          -5             25\n",
    "\n",
    "# total                                             155\n",
    "cost_round_0 = cost(hypothesis_round_0, training_y)\n",
    "cost_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19., -68.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  slope_for_theta_0 slope_for_theta_1\n",
    "# (0)(1) + 0            0  3          -3                 -3                -3\n",
    "# (0)(5) + 0            0 11         -11                -11               -55\n",
    "# (0)(2) + 0            0  5          -5                 -5               -10\n",
    "\n",
    "# total                                                 -19               -68\n",
    "\n",
    "slope_round_0 = slope(hypothesis_round_0, training_y, training_x)\n",
    "slope_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.19, 0.68]), array([0., 0., 0.]), array([-19., -68.]), 77.5)\n",
      "(array([0.3199, 1.1408]), array([0.87, 3.59, 1.55]), array([-12.99, -46.08]), 35.673750000000005)\n",
      "(array([0.409039, 1.452968]), array([1.4607, 6.0239, 2.6015]), array([ -8.9139, -31.2168]), 16.441908974999997)\n",
      "(array([0.47053039, 1.66435448]), array([1.862007, 7.673879, 3.314975]), array([ -6.149139, -21.138648]), 7.598709112657499)\n",
      "(array([0.51326612, 1.8074057 ]), array([2.13488487, 8.79230279, 3.79923935]), array([ -4.27357299, -14.30512248]), 3.5320886488925596)\n",
      "(array([0.54327568, 1.9041227 ]), array([2.32067182, 9.55029464, 4.12807753]), array([-3.000956 , -9.6716999]), 1.6616905919121403)\n",
      "(array([0.56464759, 1.96942384]), array([ 2.44739838, 10.0638892 ,  4.35152109]), array([-2.13719133, -6.53011345]), 0.8010984392836744)\n",
      "(array([0.58015426, 2.01342488]), array([ 2.53407143, 10.41176678,  4.50349527]), array([-1.55066651, -4.40010411]), 0.4048123470338611)\n",
      "(array([0.59167564, 2.04298507]), array([ 2.59357914, 10.64727865,  4.60700402]), array([-1.15213819, -2.95601955]), 0.22201805363042829)\n",
      "(array([0.60048657, 2.0627555 ]), array([ 2.63466072, 10.80660101,  4.67764579]), array([-0.88109248, -1.97704263]), 0.13739409869421815)\n",
      "(array([0.60745153, 2.07588993]), array([ 2.66324207, 10.91426407,  4.72599757]), array([-0.6964963 , -1.31344244]), 0.09791694426925235)\n",
      "(array([0.61315679, 2.08452683]), array([ 2.68334145, 10.98690116,  4.75923138]), array([-0.57052601, -0.86369001]), 0.07920687171372698)\n",
      "(array([0.61799994, 2.09011623]), array([ 2.69768361, 11.03579092,  4.78221044]), array([-0.48431503, -0.55894092]), 0.07005423985826334)\n",
      "(array([0.62225064, 2.09364137]), array([ 2.70811617, 11.06858111,  4.79823241]), array([-0.42507031, -0.35251344]), 0.06530484919157929)\n",
      "(array([0.62609181, 2.09576891]), array([ 2.71589201, 11.09045749,  4.80953338]), array([-0.38411712, -0.21275379]), 0.06258871996718074)\n",
      "(array([0.62964755, 2.09695089]), array([ 2.72186072, 11.10493635,  4.81762963]), array([-0.3555733 , -0.11819828]), 0.06081602465915498)\n",
      "(array([0.63300205, 2.09749382]), array([ 2.72659844, 11.114402  ,  4.82354933]), array([-0.33545024, -0.05429293]), 0.05948553620704203)\n",
      "(array([0.63621248, 2.09760551]), array([ 2.73049587, 11.12047115,  4.82798969]), array([-0.3210433 , -0.01116903]), 0.05836666100637261)\n",
      "(array([0.63931767, 2.09742686]), array([ 2.73381799, 11.12424003,  4.8314235 ]), array([-0.31051848,  0.01786514]), 0.057353241561189094)\n",
      "(array([0.64234399, 2.09705339]), array([ 2.73674452, 11.12645196,  4.83417138]), array([-0.30263214,  0.03734708]), 0.05639633671055326)\n",
      "(array([0.6453094 , 2.09654985]), array([ 2.73939737, 11.12761093,  4.83645076]), array([-0.29654094,  0.05035352]), 0.05547331477069584)\n",
      "(array([0.64822613, 2.09596014]), array([ 2.74185925, 11.12805866,  4.8384091 ]), array([-0.29167299,  0.05897074]), 0.05457364290875036)\n",
      "(array([0.65110253, 2.09531401]), array([ 2.74418627, 11.12802685,  4.84014642]), array([-0.28764046,  0.06461336]), 0.053692353238528806)\n",
      "(array([0.65394433, 2.09463161]), array([ 2.74641654, 11.12767259,  4.84173055]), array([-0.28418032,  0.06824059]), 0.05282703857900568)\n",
      "(array([0.65675548, 2.09392658]), array([ 2.74857594, 11.12710236,  4.84320755]), array([-0.28111415,  0.07050284]), 0.05197647110874393)\n",
      "(array([0.65953869, 2.09320817]), array([ 2.75068205, 11.12638836,  4.84460863]), array([-0.27832096,  0.07184112]), 0.05113996720838795)\n",
      "(array([0.66229587, 2.09248262]), array([ 2.75274685, 11.12557951,  4.84595502]), array([-0.27571862,  0.07255446]), 0.050317095391747106)\n",
      "(array([0.66502839, 2.09175417]), array([ 2.75477849, 11.12470898,  4.84726111]), array([-0.27325142,  0.07284561]), 0.04950754198528758)\n",
      "(array([0.6677372 , 2.09102564]), array([ 2.75678255, 11.12379921,  4.84853672]), array([-0.27088152,  0.07285204]), 0.04871104933889441)\n",
      "(array([0.67042303, 2.09029898]), array([ 2.75876285, 11.12286542,  4.84978849]), array([-0.26858324,  0.07266695]), 0.04792738738610589)\n",
      "(array([0.67308642, 2.08957544]), array([ 2.76072201, 11.12191791,  4.85102098]), array([-0.2663391 ,  0.07235352]), 0.047156340538501304)\n",
      "(array([0.6757278 , 2.08885589]), array([ 2.76266186, 11.12096362,  4.8522373 ]), array([-0.26413721,  0.07195459]), 0.04639770163101926)\n",
      "(array([0.67834749, 2.0881409 ]), array([ 2.76458369, 11.12000727,  4.85343958]), array([-0.26196946,  0.07149919]), 0.04565126910967122)\n",
      "(array([0.68094579, 2.08743083]), array([ 2.76648839, 11.119052  ,  4.8546293 ]), array([-0.25983031,  0.07100699]), 0.04491684571052215)\n",
      "(array([0.68352295, 2.08672592]), array([ 2.76837663, 11.11809996,  4.85580746]), array([-0.25771596,  0.07049132]), 0.04419423782479241)\n",
      "(array([0.68607919, 2.08602631]), array([ 2.77024887, 11.11715255,  4.85697479]), array([-0.25562379,  0.0699612 ]), 0.04348325517986677)\n",
      "(array([0.68861471, 2.08533208]), array([ 2.7721055 , 11.11621073,  4.85813181]), array([-0.25355197,  0.06942274]), 0.04278371066599525)\n",
      "(array([0.6911297 , 2.08464328]), array([ 2.77394679, 11.11527511,  4.85927887]), array([-0.25149923,  0.06888008]), 0.04209542023040337)\n",
      "(array([0.69362435, 2.08395992]), array([ 2.77577298, 11.1143461 ,  4.86041626]), array([-0.24946466,  0.06833599]), 0.041418202802823635)\n",
      "(array([0.69609883, 2.083282  ]), array([ 2.77758427, 11.11342394,  4.86154419]), array([-0.2474476 ,  0.06779237]), 0.04075188023588484)\n",
      "(array([0.6985533 , 2.08260949]), array([ 2.77938082, 11.1125088 ,  4.86266282]), array([-0.24544756,  0.06725047]), 0.0400962772527432)\n",
      "(array([0.70098794, 2.08194238]), array([ 2.78116279, 11.11160075,  4.86377228]), array([-0.24346417,  0.06671113]), 0.039451221398444786)\n",
      "(array([0.70340291, 2.08128063]), array([ 2.78293032, 11.11069984,  4.8648727 ]), array([-0.24149714,  0.06617492]), 0.03881654299339538)\n",
      "(array([0.70579838, 2.08062421]), array([ 2.78468354, 11.10980606,  4.86596417]), array([-0.23954622,  0.06564222]), 0.038192075088194284)\n",
      "(array([0.70817449, 2.07997308]), array([ 2.78642258, 11.10891942,  4.86704679]), array([-0.23761121,  0.06511325]), 0.037577653419471176)\n",
      "(array([0.71053141, 2.07932719]), array([ 2.78814756, 11.10803987,  4.86812064]), array([-0.23569193,  0.06458817]), 0.0369731163665652)\n",
      "(array([0.71286929, 2.07868652]), array([ 2.7898586 , 11.10716738,  4.8691858 ]), array([-0.23378823,  0.06406707]), 0.0363783049089591)\n",
      "(array([0.71518829, 2.07805102]), array([ 2.79155581, 11.1063019 ,  4.87024234]), array([-0.23189995,  0.06355001]), 0.03579306258442545)\n",
      "(array([0.71748856, 2.07742065]), array([ 2.79323931, 11.1054434 ,  4.87129034]), array([-0.23002695,  0.063037  ]), 0.03521723544785927)\n",
      "(array([0.71977025, 2.07679537]), array([ 2.79490921, 11.10459182,  4.87232986]), array([-0.2281691 ,  0.06252806]), 0.03465067203077876)\n",
      "(array([0.72203351, 2.07617514]), array([ 2.79656562, 11.10374711,  4.87336099]), array([-0.22632627,  0.06202317]), 0.0340932233014791)\n",
      "(array([0.7242785 , 2.07555992]), array([ 2.79820865, 11.10290922,  4.87438379]), array([-0.22449834,  0.06152232]), 0.03354474262583064)\n",
      "(array([0.72650535, 2.07494966]), array([ 2.79983841, 11.10207808,  4.87539833]), array([-0.22268517,  0.06102549]), 0.033005085728705925)\n",
      "(array([0.72871421, 2.07434434]), array([ 2.80145501, 11.10125366,  4.87640467]), array([-0.22088666,  0.06053266]), 0.03247411065602876)\n",
      "(array([0.73090524, 2.0737439 ]), array([ 2.80305855, 11.10043589,  4.87740289]), array([-0.21910267,  0.06004379]), 0.031951677737434604)\n",
      "(array([0.73307857, 2.07314831]), array([ 2.80464914, 11.09962473,  4.87839304]), array([-0.21733309,  0.05955887]), 0.031437649549528976)\n",
      "(array([0.73523435, 2.07255753]), array([ 2.80622688, 11.09882012,  4.87937519]), array([-0.21557781,  0.05907786]), 0.03093189087974216)\n",
      "(array([0.73737272, 2.07197152]), array([ 2.80779188, 11.098022  ,  4.88034941]), array([-0.2138367 ,  0.05860072]), 0.03043426869075852)\n",
      "(array([0.73949381, 2.07139025]), array([ 2.80934424, 11.09723033,  4.88131576]), array([-0.21210966,  0.05812744]), 0.029944652085524454)\n",
      "(array([0.74159778, 2.07081367]), array([ 2.81088406, 11.09644506,  4.88227431]), array([-0.21039657,  0.05765798]), 0.02946291227281517)\n",
      "(array([0.74368475, 2.07024175]), array([ 2.81241145, 11.09566613,  4.88322512]), array([-0.20869731,  0.05719231]), 0.028988922533357355)\n",
      "(array([0.74575487, 2.06967444]), array([ 2.8139265 , 11.09489348,  4.88416824]), array([-0.20701177,  0.0567304 ]), 0.028522558186495774)\n",
      "(array([0.74780827, 2.06911172]), array([ 2.81542931, 11.09412708,  4.88510375]), array([-0.20533985,  0.05627222]), 0.02806369655739726)\n",
      "(array([0.74984508, 2.06855354]), array([ 2.81691999, 11.09336687,  4.88603171]), array([-0.20368143,  0.05581775]), 0.027612216944781384)\n",
      "(array([0.75186545, 2.06799987]), array([ 2.81839863, 11.09261279,  4.88695217]), array([-0.20203641,  0.05536694]), 0.027168000589170793)\n",
      "(array([0.75386949, 2.06745068]), array([ 2.81986532, 11.09186481,  4.88786519]), array([-0.20040467,  0.05491977]), 0.026730930641651034)\n",
      "(array([0.75585736, 2.06690591]), array([ 2.82132017, 11.09112287,  4.88877084]), array([-0.19878612,  0.05447621]), 0.026300892133136335)\n",
      "(array([0.75782916, 2.06636555]), array([ 2.82276327, 11.09038692,  4.88966918]), array([-0.19718063,  0.05403624]), 0.025877771944125067)\n",
      "(array([0.75978504, 2.06582955]), array([ 2.82419471, 11.08965692,  4.89056026]), array([-0.19558811,  0.05359982]), 0.025461458774945826)\n",
      "(array([0.76172513, 2.06529788]), array([ 2.8256146 , 11.08893281,  4.89144415]), array([-0.19400845,  0.05316692]), 0.02505184311647978)\n",
      "(array([0.76364954, 2.06477051]), array([ 2.82702301, 11.08821454,  4.89232089]), array([-0.19244155,  0.05273752]), 0.024648817221355503)\n",
      "(array([0.76555842, 2.06424739]), array([ 2.82842005, 11.08750208,  4.89319056]), array([-0.19088731,  0.05231159]), 0.02425227507560593)\n",
      "(array([0.76745187, 2.0637285 ]), array([ 2.82980581, 11.08679538,  4.8940532 ]), array([-0.18934561,  0.0518891 ]), 0.02386211237078213)\n",
      "(array([0.76933004, 2.0632138 ]), array([ 2.83118037, 11.08609438,  4.89490887]), array([-0.18781637,  0.05147002]), 0.023478226476515752)\n",
      "(array([0.77119303, 2.06270326]), array([ 2.83254384, 11.08539904,  4.89575764]), array([-0.18629948,  0.05105432]), 0.02310051641352218)\n",
      "(array([0.77304098, 2.06219684]), array([ 2.83389629, 11.08470932,  4.89659955]), array([-0.18479484,  0.05064198]), 0.022728882827039068)\n",
      "(array([0.774874  , 2.06169451]), array([ 2.83523782, 11.08402517,  4.89743466]), array([-0.18330236,  0.05023298]), 0.02236322796069035)\n",
      "(array([0.77669222, 2.06119624]), array([ 2.83656851, 11.08334654,  4.89826302]), array([-0.18182193,  0.04982727]), 0.022003455630773633)\n",
      "(array([0.77849576, 2.06070199]), array([ 2.83788846, 11.0826734 ,  4.89908469]), array([-0.18035345,  0.04942484]), 0.021649471200957806)\n",
      "(array([0.78028472, 2.06021173]), array([ 2.83919774, 11.08200569,  4.89989973]), array([-0.17889683,  0.04902567]), 0.021301181557390844)\n",
      "(array([0.78205924, 2.05972543]), array([ 2.84049646, 11.08134338,  4.90070819]), array([-0.17745198,  0.04862971]), 0.020958495084205827)\n",
      "(array([0.78381943, 2.05924306]), array([ 2.84178468, 11.08068641,  4.90151011]), array([-0.1760188 ,  0.04823696]), 0.020621321639421902)\n",
      "(array([0.7855654 , 2.05876459]), array([ 2.8430625 , 11.08003475,  4.90230556]), array([-0.17459719,  0.04784737]), 0.020289572531233282)\n",
      "(array([0.78729728, 2.05828998]), array([ 2.84432999, 11.07938835,  4.90309458]), array([-0.17318707,  0.04746094]), 0.019963160494678896)\n",
      "(array([0.78901516, 2.0578192 ]), array([ 2.84558726, 11.07874718,  4.90387724]), array([-0.17178833,  0.04707762]), 0.019641999668688327)\n",
      "(array([0.79071917, 2.05735223]), array([ 2.84683436, 11.07811118,  4.90465357]), array([-0.17040089,  0.0466974 ]), 0.019326005573495418)\n",
      "(array([0.79240941, 2.05688903]), array([ 2.8480714 , 11.07748032,  4.90542363]), array([-0.16902465,  0.04632025]), 0.019015095088417696)\n",
      "(array([0.79408601, 2.05642957]), array([ 2.84929844, 11.07685455,  4.90618747]), array([-0.16765953,  0.04594615]), 0.018709186429990896)\n",
      "(array([0.79574906, 2.05597382]), array([ 2.85051558, 11.07623384,  4.90694514]), array([-0.16630544,  0.04557507]), 0.01840819913045627)\n",
      "(array([0.79739869, 2.05552175]), array([ 2.85172288, 11.07561814,  4.9076967 ]), array([-0.16496228,  0.04520698]), 0.018112054016594506)\n",
      "(array([0.79903499, 2.05507333]), array([ 2.85292043, 11.07500742,  4.90844218]), array([-0.16362997,  0.04484187]), 0.017820673188898864)\n",
      "(array([0.80065807, 2.05462853]), array([ 2.85410831, 11.07440162,  4.90918164]), array([-0.16230842,  0.04447971]), 0.017533980001085164)\n",
      "(array([0.80226805, 2.05418733]), array([ 2.8552866 , 11.07380072,  4.90991513]), array([-0.16099755,  0.04412047]), 0.01725189903992901)\n",
      "(array([0.80386502, 2.05374968]), array([ 2.85645537, 11.07320467,  4.9106427 ]), array([-0.15969726,  0.04376413]), 0.01697435610542992)\n",
      "(array([0.80544909, 2.05331558]), array([ 2.8576147 , 11.07261344,  4.91136439]), array([-0.15840747,  0.04341067]), 0.016701278191292288)\n",
      "(array([0.80702037, 2.05288498]), array([ 2.85876467, 11.07202698,  4.91208025]), array([-0.1571281 ,  0.04306007]), 0.016432593465722664)\n",
      "(array([0.80857896, 2.05245785]), array([ 2.85990535, 11.07144526,  4.91279033]), array([-0.15585906,  0.0427123 ]), 0.01616823125253359)\n",
      "(array([0.81012497, 2.05203418]), array([ 2.86103682, 11.07086823,  4.91349467]), array([-0.15460028,  0.04236733]), 0.015908122012553483)\n",
      "(array([0.81165848, 2.05161393]), array([ 2.86215915, 11.07029587,  4.91419333]), array([-0.15335165,  0.04202515]), 0.01565219732533402)\n",
      "(array([0.81317962, 2.05119707]), array([ 2.86327241, 11.06972813,  4.91488634]), array([-0.15211312,  0.04168574]), 0.015400389871152885)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.81317962, 2.05119707]),\n",
       " array([ 2.86327241, 11.06972813,  4.91488634]),\n",
       " array([-0.15211312,  0.04168574]),\n",
       " 0.015400389871152885)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum(training_x, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our result with the correct $\\theta_0 = 1$ and $\\theta_1 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d426c71460>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAglElEQVR4nO3debxM9ePH8dfn7vZ9X7oKKRRCQlK2kqUiX6ESJSot4hsqRQlFpJJEkaQFkSwJCT9lV/bse5ZsWe7++f0xY8b3ZrncuffM8n4+Hh7NZz6H836czNu5M2fOx1hrERGRwBPmdAAREbk6KnARkQClAhcRCVAqcBGRAKUCFxEJUBGZubP8+fPb2NjYzNyliEjAW7ly5RFrbYHUz2dqgcfGxrJixYrM3KWISMAzxuy60PN6C0VEJECpwEVEApQKXEQkQKnARUQClApcRCRAqcBFRAKUClxEJECpwEVEMtKuX2HZJ5ABt+7O1C/yiIiEjOQkGFkLDm9yjSu1gahsPt2FzsBFRHxt8yx4I5+nvLtGv0mcifH5bnQGLiLiK4lxMKQsxJ0AYEnyjbRJfBniDO+F+/58WQUuIuILqyfAtKc8w8bxb7HBxlKrdD7Gd7iVsDDj812qwEVE0iPuBAws6RlOTa7J84nPADC3Wx1KF8yRYbtWgYuIXK3Fw2Dua55hnfih7LaF6FTnWno3viHDd68CFxG5Uv8cdL3X7TYq6V7eSmoLwKpXG5A3W1SmxFCBi4hciR9fhl8/8AyrxY3gMLl5p+VNPFi1RKZGUYGLiKTF0e0wvLJn+FbiQ4xKbkqJvFn4s1tdoiIy/6psFbiIyOVM6gjrJnmGFeNG8w9Z+bbzbVSLzetYLBW4iMjFHPgdPq7jGXZPfJJJyXdwT4XCjGhbBWN8f2nglVCBi4ikZi2MbQK7FgNw3Gbj1vgPiSeKhT3upGS+rA4HdFGBi4icb8ciGNfEM+yY8CLzUm6he8OyPHNXGQeD/ZsKXEQEXDef+rA6HN0GwOaU4jROGEAy4ax9vSE5YiIdDvhvKnARkY3T4et2nmHL+D6ssOX4qG0V7qlYxMFgl6YCF5HQlXAG3rkOEs8AsDC5Io8k9qRCsVxsfaoWERlwAypfUoGLSGhaOQ6mP+sZNoofyGZbkh+61qZCsVwOBks7FbiIhJazx2BQrGc4KbkO3RM707paCX5scZNzua6CClxEQseiITCvn2dYO34Ye21BlvauR6Gcvl9wIaOpwEUk+J08AO+W8wxHJDXj7aTWvN70RtrXKuVgsPRRgYtIcJv1Eiwd6RneEvcRSVnys7FPPbJEhTsYLP1U4CISnI5shQ9u8Qz7JT7Mp8n3MK5Dde4oW8DBYL6jAheR4GItfPsobJjmeap83BgqlS7O9gxa2swply1wY8ynQBPgkLW2gvu5vMDXQCywE2hlrT2WcTFFRNJg/2oYVdczfD7hKaam1M7wpc2ckpar1McCd6d6ricwz1pbBpjnHouIOCMlBcY09JT3YZuTsnHjKFT7EXYOvDcoyxvScAZurV1ojIlN9XRzoK778ThgAfCSL4OJiKTJ9gXweXPPsH1CDxakVGb1qw3Ik0lLmznlat8DL2StPeB+/BdQ6GIbGmM6AZ0ASpYsebHNRESuTHIiDK8CJ3YDsC4llmYJb/L2g5UZe0txh8NljnR/iGmttcYYe4n5UcAogKpVq150OxGRNFv/HXzb3jO8P74vR/LcxCaHljZzytUW+EFjTBFr7QFjTBHgkC9DiYhcUMJpGHgNpCQCMC+5Mh0Tu/Nt55qOLm3mlKst8O+BR4GB7v9Ou/TmIiLptHw0zHjRM6wf/zbXV6jGjjaVHV/azClpuYxwIq4PLPMbY/YCr+Eq7m+MMR2BXUCrjAwpIiHszFF42/t194lJd9Ir6QkW/fdOSuT1j6XNnJKWq1AeushUPR9nERH5XwsGwoIBnmHNuOG0bVSLnXeWdjCU/9A3MUXE/5zYC0PLe4bvJT3A0KSWfru0mVNU4CLiX37oBivGeIaV40YyoF1ddlbw36XNnKICFxH/cHiza1Fhtz6Jj7K6cCtWPF2L8CC6f4kvqcBFxFnWwldtYfMMAFKsoUL8GL7pWp9+AbK0mVNU4CLinL0rYfRdnmHXhGfIXrU1Gx6o6GCowKECF5HMl5LiKu79qwE4YPNSJ34Yi3s3CsilzZyiAheRzLV1LnzRwjNsl9CLBk1as6VmrHOZApQKXEQyR1ICdlhFzKm/AFidUprHwvrzW98GxEQG9tJmTlGBi0jGWzsJJnfk3LUkzeP78eJjbVgTJEubOUUFLiIZJ/4UDCjmGc5OrsaEa95kasdbQ/b+Jb6kAheRjLF0FMzq4RnWi3+Hj19ow/iC2R0MFVxU4CLiW6ePwDvXeYbjk+qzr1Z/5t1TzsFQwUkFLiI+Y+e9gVk02DOuEfc+s15tHfRLmzlFBS4i6Xd8Nwyr6PmQckhiS2If6MtvIbK0mVNU4CKSLslTnyZ8zReecZOs45nSrUlILW3mFBW4iFydQxthRA3OXcH9cmIH7n/iVX4IwaXNnKICF5ErYy0Jn7cgasc8AOJtJD3LTOPdtjV1aWAmU4GLSNrtWQZjGnDuI8kuCc/Ru3tPhob40mZOUYGLyOWlJBP3YS1i/t4IwK6Ugsy843s+qneDw8FCmz5lEJFLStn8I/TL6ynvNgkvk7f3BrqovB2nM3ARubCkeBLeuZ6o+GMALEu5nqMPTuXLikUdDibnqMBF5F/iV04gevpTnve6X8g9jMHPttfSZn5GBS4iXnEnYWAJot3DH5JrEPvk1wwtntvJVHIRKnARAeDE/KHkWvi6Zzz4+ol0f6ixY3nk8lTgIqHu1CEYXIZzywd/mnQ3Tf47ju5a2szvqcBFQtjByS9RaO1Iz/jbunPpULeag4nkSqjARUJQ0pEdRHxQiULu8fumDU/0/oAHtbRZQFGBi4SYPWMeocSeaZ7xkgdX0bX8dZf4HeKv9EUekRDxz6418HouT3mPzvMC9rXj1FR5ByydgYsEO2vZNawB15xYDsA/NguHnlzL40W1oHCg0xm4SBA78Md86JvbU96Ty7xNjr5/cZ3KOyjoDFwkCNnkRA4MrELRxN0AbEspQt4eq2iRQ3cNDCYqcJEgs2nB15Rb0IlzdyxZWHMsdRre72gmyRgqcJEgEX/2FAmDylCOMwCsCqtIhV6/UEeXBgYtFbhIEPht8nvUWNvHcw+TDc1mUKVKbUczScZTgYsEsL+PHCLfB2Wo4R4vy1Gfat0mcaOWNgsJKnCRAPXTqF402D/CM97/6G9UL6VFFkKJClwkwGzZtoUy46vSwD1eXeJhKnf8AC2zEHpU4CIBIiXF8sPgDjQ7M8Xz3OmuG6icr5iDqcRJ6foijzHmBWPMemPMOmPMRGOM7j8pkgGWLFtGWL/cnvLeVLEHvH6CbCrvkHbVZ+DGmGLAs8CN1tqzxphvgNbAWB9lEwl5p+OT+Ll/U5qELfE8l/zfXZTLmtu5UOI30vtV+gggizEmAsgK7E9/JBEBmDJjJtkG5POU9947hsDrJwhXeYvbVZ+BW2v3GWMGA7uBs8Aca+2c1NsZYzoBnQBKlix5tbsTCRn7j51mz9C7eCBsEwBnwnOStecWikfqHUr5X1d9Bm6MyQM0B0oBRYFsxph2qbez1o6y1la11lYtUEA30BG5lA8//Yyi7xXlVnd5H79vPFlf3QMqb7mA9FyFUh/YYa09DGCMmQLUBL7wRTCRULJ65yFyf1qbp8MOAnAse2nyvLCU3OG6UEwuLj1/O3YDNYwxWXG9hVIPWOGTVCIhIik5hf6DB/Ha2YGen4fjH5lJnmtrORtMAkJ63gNfaoyZBKwCkoDVwChfBRMJdjNWbuOu72vwmkkA4GiR28nbaTrR+hq8pFG6fj6z1r4GvOajLCIh4cSZRAb078nAyNHg7mrbZQl5C5V3NpgEHL3BJpKJPpy5nKeX1WdgpGt8slwrcrb+BJ1zy9VQgYtkgh1HTjNp6PP0iPzG++Rzf5AzzzXOhZKApwIXyUDWWl4cM5t397amh/usO67Gc8Tc3c/ZYBIUVOAiGWTJtiNs/uwp3o340ftk963EZNf3IcQ3VOAiPhaXmEzbQROYnNSVmu5XWHLDtwiv+bSzwSToqMBFfGji0l3k+uFxJocv8z7Zay/h0TmcCyVBSwUu4gOH/4mnw1ujmB79CrjXELb3j8Lc/B9ng0lQU4GLpFPfaWtpsvIxpkdvASA5awHCu63HRERf5neKpI8KXOQqbfrrJG8MH8GEqAHe28K1nUx4mfqO5pLQoQIXuUIpKZbWH/3Cu4c6MiHqCADJhW4i/MkFEBbubDgJKSpwkSswd8NBpnzxAd9EDfd8DZ6OcwkvUc3RXBKaVOAiaXA6Poma/aazMqID9aNSALBl78Y89BXo5lPiEBW4yGWMWriN3T++z++Rn3mffGoppmA550KJoAIXuah9x89y78BprIl5Etxfg+eW9tD0PSdjiXiowEVSsdbywtdriF03nDUxU7wTz6+D3CWcCyaSigpc5Dyrdx/jqRHT+TWmq/fVccdLcGdvR3OJXIgKXARITE7h7mELeezYcH6Nmeed6LEdsuVzLpjIJajAJeRNW7OP4V/PYF50D+8rovFgqP6Eo7lELkcFLiHrxJlEbu73I59EDmFe9CoArAnH9NwN0dkdTidyeSpwCUlD5mxm0c+z2RnTx/tky08xFVo4F0rkCqnAJaTsOHKaeoPnMzXqVV6M3uF6MmdxeHY1REQ5G07kCqnAJSRYa+kwdjnJW+ayPWaQd+Lh7+C6u5wLJpIOKnAJeku2HqH96MUsjn6OglHHXU8Wqwodf4KwsEv+XhF/pgKXoBWXmEztQfOpeeZn/oz50DvxxHwodotzwUR8RAUuQenLpbvp/90y1sd0hHNvbd/QFFqN182nJGiowCWoHP4nnmr959I+fDbrYz73TjyzAvKXcS6YSAZQgUvQeG3aOqb/upadMZ29T1Z7HO4d4lwokQykApeAt/HASe55bxEvRnzDqpip3okXNkCuYo7lEsloKnAJWCkplpYjl3Bw9xZ2xjznnbjzFbijh3PBRDKJClwC0k8bDvLE5ysYFDGK/8Qs8E78dwdkzetYLpHMpAKXgHI6PolK/eZQKmU3O2Ne8k40GQpVOzgXTMQBKnAJGB//so0BszYyLnIQd0T+4XoyIsZ11h2V1dlwIg5QgYvf23f8LLUGzqeK+ZOdMa97J1p9Djc2dyyXiNNU4OK3rLU8//Uapq/Zy6yoXtwQtsc1kacUPLMcwiMv/QeIBDkVuPilVbuP8cCIJdwZtprtMe94Jx6dDqXqOBdMxI+owMWvJCan0GjYQvYdPsaq6GfIa065JkrWhPYzdPMpkfOowMVvTF29j+e/XsMDYQuZHzPSO9HpFyhaybFcIv5KBS6Ocy1tNofsnGFnzOPeifIPQMtPdfMpkYtQgYujBv+4mQ9+3srj4TN4JXKCd6LrKsh3nXPBRAJAugrcGJMbGA1UACzQwVr7qw9ySZDbfvgUdw35hfycYGdMF+/ErV3gnoHOBRMJIOk9A38PmG2tbWmMiQL0bQq5JGst7T9bzi9/HqZnxEQ6R0z3Tr64GXIUdi6cSIC56gI3xuQC6gDtAay1CUCCb2JJMPq/rUdoO3opJcxBdsa84J2o/zrUfuGiv09ELiw9Z+ClgMPAZ8aYm4GVwHPW2tPnb2SM6QR0AihZsmQ6dieBKi4xmVoD5/P36QSGRI6gRfhi7+RLuyBLbseyiQSy9FxUGwFUAT6y1lYGTgM9U29krR1lra1qra1aoECBdOxOAtGEpbso9+psCp7Zws6YNt7ybvY+vH5C5S2SDuk5A98L7LXWLnWPJ3GBApfQdOifOKr3nwdYJkS+Ra3w9a6JqBzQYwtEZnE0n0gwuOoCt9b+ZYzZY4y53lq7GagHbPBdNAlUr05dx/jfdlHNbOLb6H7eidZfQrl7nQsmEmTSexVKV2CC+wqU7cBj6Y8kgWrD/pM0Hr6IcJKZG/USpcP2uybyl4Uuv0K4vnYg4kvpekVZa9cAVX0TRQJVsntps9W7j1M/bCWjo85bRLj9TIit5Vw4kSCmUyJJlznr/6LT+JVEk8Da6C7kMGddE6XqwCPf62vwIhlIBS5X5VR8EpX6ziEpxfJg+ALeiRzlney8GApXdCybSKhQgcsVG/nLNgbO2kROTvNHzBPeiYqtoMUnzgUTCTEqcEmzvcfOUHvQzwB0Dv+enpFfeSefXQN5SzkTTCREqcDlsqy1PPvVGqb/vp8CHGN5zNPeyZrPQsM3nAsnEsJU4HJJK3cdo8VHSwB4JWI8j0fM8k523wLZCzqUTERU4HJBickpNBq6kO1HTnON+Ytfort5Jxu+CTW7OhdORAAVuFzAuaXNwPJ+5Ps0Df/NO9lzN8TkciqaiJxHBS4ex88kUKnfTwCUNzuZEd3bO3nfR1CpjUPJRORCVOACwNuzNzFiwTYMKXwV9Sa3hm1yTWTJC902QmSMswFF5F9U4CHu3NJmALeFrWdiVH/vZJtvoGwjh5KJyOWowEOUtZZHP1vOwj8PE0ESc6N6EBt20DVZsDx0XgRh4c6GFJFLUoGHoMVbjtBujOs27neHLWNk1DDvZIcfoWQNZ4KJyBVRgYeQuMRkag6cz9HTCcQQz+8xTxJ9bhnT6+pBu8m6+ZRIAFGBh4jxv+3i1anrAHgofB4DIsd4J7v8CoVudCiZiFwtFXiQ8y5tBrk4xe8xnbyTldrBfR86lExE0ksFHsRembqWL37bDcDT4VPpEfmNd/K5PyDPNQ4lExFfUIEHoXNLmwEU4ihLY57xTt7+ItTr41AyEfElFXgQSUmxfLNiDz2nrAXgzaixtAub492gxzbIlt+hdCLiayrwILH10Cl6f7eWZTuOclvOo0xMOO+s++6BUKOLc+FEJEOowANcfFIyIxds58Oft5IlMoyJt+3jttU9vBv02gvROZwLKCIZRgUewJbvPEqvKWvZeugUbctH0YfRRK+eDTG5ofFguOlBpyOKSAZSgQegE2cTeXv2JiYs3U2xXDHMvn0H5f54G5LjXffqvrULhOt/rUiw06s8gFhrmb3uL177fj1HTsXTo2oknf8ZSvjyRXBNbWg2HPJd53RMEckkKvAAsf/4WfpMW8/cjQepWCQb06qsocjKwRAWAU2GQZVHISzM6ZgikolU4H4uOcUy/tedvPPjZpKt5Z06EbTc2w+zdCWUaQRNhkKuYk7HFBEHqMD92MYDJ+k5ZS2/7znOnWVyM6zofHItfw9ickKLMVChhW4+JRLCVOB+KC4xmffmbeGThdvJlSWScY3CqbOxO2bpBqj4oOu6bn0hRyTkqcD9zOItR3h56lp2/X2GNpXz0yfbVGIWjoTsheGhr+D6e5yOKCJ+QgXuJ46eTqD/jI1MXrWX2HxZ+aFJChVWPgnHdsAtj0GDvloNXkT+hwrcYdZapq7Zxxs/bOTk2US63V6Yp5LGETF3HOQpBY9Oh1J1nI4pIn5IBe6g3X+f4eWpa1m05QiVS+bm/SoHKf5/beHUQajZFer2hqisTscUET+lAndAYnIKYxbvYNjcP4kIC2PQPUV48PCHhM2e5FpQuPUEKHaL0zFFxM+pwDPZ73uO03PKWjYeOEnDGwoyqNyf5FnQGeL/cZ1x134BIqKcjikiAUAFnklOxycxZM6fjF2ygwI5ovnsgWLcuXUAzJoNxapC8w+g4A1OxxSRAKICzwTzNx3k1anr2X/iLA9XL0GvwkvJMrc9pCRBo7fg1s4QFu50TBEJMCrwDHTonzj6Tt/AjD8OULZQdqY/VJgKK3vB74tdV5Y0HQ55SzkdU0QClAo8A6SkWL5esYcBMzcSl5RCjwbX8mTUHCK+fwvCo1zFXeURfQ1eRNJFBe5jWw+doveUtSzbeZQa1+Zl8O3hFF/UBfavhusbw71DIGdRp2OKSBBId4EbY8KBFcA+a22T9EcKTPFJyXy0YBsjft5GlqhwBt9XjhZnvsJ8+65rhZyWn0H5+3XWLSI+44sz8OeAjUBOH/xZAen8pc2a3VyUvlXOkGduGzi8CW76j+vmU1nzOh1TRIJMugrcGFMcuBfoD3TzSaIAcuJsIoNmb+LLpbspljsL4x4uzx17PoaJH7neJmnzLZRt6HRMEQlS6T0DHwb8FwipZc+ttcxyL23296l4nri9FC+WPkDMrGZwfBdU7Qj1X3fdt1tEJINcdYEbY5oAh6y1K40xdS+xXSegE0DJkiWvdnd+w7W02TrmbjxE+aI5Gdu6LOXXvQ0Tx0Pe66D9TIit5XRMEQkB6TkDrwU0M8Y0BmKAnMaYL6y17c7fyFo7ChgFULVqVZuO/TkqOcXy+a87GfzjZlIsvNz4Bjrk30D4dw3g9GGo9TzU7QmRWZyOKiIh4qoL3FrbC+gF4D4D7566vIPF+Uub3VG2AG81LESxX1+D+d9BoYrQ5isoWtnpmCISYnQd+CWkXtrsvf/cTDOzCDOhFSSchrtecZ15h0c6HVVEQpBPCtxauwBY4Is/y1+cv7RZq6rFebl2DnLNfR62/gTFq7tuPlXgeqdjikgI0xl4KkdPJ/DmjA1MWbWP2HxZ+fLxatQ8Og0+fR1sCtw9CKo/oZtPiYjjVOBuqZc2e+bO0nS9GaJnPgq7l8C1daHpe5An1umoIiKAChyAXX+f5pWp6zxLmw2870au3z4WPhkAkTHQ/EOo1FZfgxcRvxLSBZ56abM3mpen7TUnCZveHA78DuWauG4+laOw01FFRP4lZAv8/KXNGpUvRN/GpSm85n0YPQyy5IVWn8ONzZ2OKSJyUSFX4KfikxgyZzPjluykQI5oRra7hbtz7oKJDeDIn3BzG2jUXzefEhG/F1IFPm/jQV6duo4DJ+Nod+s19LirGDkXD4BloyBXcWg3GUrXdzqmiEiahESBHzrpXtpsrWtps0ltbuOWxNUw5iE4sRuqd4J6fSA6pO7JJSIBLqgL/NzSZm/N3Eh8UgrdG5alU7V8RM17BdZMgHxl4LHZcM1tTkcVEbliQVvgqZc2e+v+ilx7eD583B1OH4Ha3eCOl1yXCYqIBKCgK/DUS5u93fImHrw+EjOzC2z8HgpXhLbfQpGbnY4qIpIuQVXgy3YcpdeUP9h2+DTNbi5KnyY3kH/rZPiwNySedb3PXfNZ3XxKRIJCUBT4ibOJDJy1iYnLdlM8TxbGPlaNugXPwtSHYNt8KFEDmr0PBco6HVVExGcCusAvtLTZC/VLk3XNZzCpr2uje96Bao9DWJizYUVEfCxgC/z8pc0qFMvJZ+2rUSHqIHzRFPb8BtfVg6bDIHfgL+MmInIhAVfgqZc2e+XeG2h/azEifnsffhkEkVnhvpFwc2vdfEpEglpAFfiG/SfpNeUPft97gjvKFuDN+ypQIu5P+PQR+Gut694ljQdD9oJORxURyXABUeBnE9xLmy3aTp6skQx/qDJNb8iNWfg2/N9wyJYfWo2HG5s5HVVEJNMERIG3/2wZS3ccpVXV4vRufAO5D6+Ej5vC31uhUjto9CZkyeN0TBGRTBUQBf7MXaV5LsxQs3g0zO0Nyz9xfTj58Hdw3V1OxxMRcURAFPjtZQrAlrkw4nk4sRdu7eJaET46u9PRREQcExAFzvTnYOVYyH89dJwDJao7nUhExHGBUeB5r4U6PVy/IqKdTiMi4hcCo8BrPed0AhERv6Pvl4uIBCgVuIhIgFKBi4gEKBW4iEiAUoGLiAQoFbiISIBSgYuIBCgVuIhIgDLW2szbmTGHgV1X+dvzA0d8GMdXlOvKKNeVUa4rE6y5rrHWFkj9ZKYWeHoYY1ZYa6s6nSM15boyynVllOvKhFouvYUiIhKgVOAiIgEqkAp8lNMBLkK5roxyXRnlujIhlStg3gMXEZH/FUhn4CIich4VuIhIgPKrAjfGfGqMOWSMWXeReWOMGW6M2WqM+cMYU8VPctU1xpwwxqxx/+qTSblKGGN+NsZsMMasN8b8a+ULJ45ZGnNl+jEzxsQYY5YZY3535+p7gW2ijTFfu4/XUmNMrJ/kam+MOXze8Xo8o3Odt+9wY8xqY8wPF5jL9OOVxlyOHC9jzE5jzFr3PldcYN63r0drrd/8AuoAVYB1F5lvDMwCDFADWOonueoCPzhwvIoAVdyPcwB/Ajc6fczSmCvTj5n7GGR3P44ElgI1Um3zFDDS/bg18LWf5GoPfJDZf8fc++4GfHmh/19OHK805nLkeAE7gfyXmPfp69GvzsCttQuBo5fYpDnwuXX5DchtjCniB7kcYa09YK1d5X78D7ARKJZqs0w/ZmnMlencx+CUexjp/pX6U/zmwDj340lAPWOM8YNcjjDGFAfuBUZfZJNMP15pzOWvfPp69KsCT4NiwJ7zxnvxg2Jwu839I/AsY0z5zN65+0fXyrjO3s7n6DG7RC5w4Ji5f+xeAxwCfrLWXvR4WWuTgBNAPj/IBdDC/WP3JGNMiYzO5DYM+C+QcpF5R45XGnKBM8fLAnOMMSuNMZ0uMO/T12OgFbi/WoXrXgU3A+8DUzNz58aY7MBk4Hlr7cnM3PelXCaXI8fMWptsra0EFAeqG2MqZMZ+LycNuaYDsdbam4Cf8J71ZhhjTBPgkLV2ZUbv60qkMVemHy+32tbaKsA9wNPGmDoZubNAK/B9wPn/khZ3P+coa+3Jcz8CW2tnApHGmPyZsW9jTCSukpxgrZ1ygU0cOWaXy+XkMXPv8zjwM3B3qinP8TLGRAC5gL+dzmWt/dtaG+8ejgZuyYQ4tYBmxpidwFfAXcaYL1Jt48Txumwuh44X1tp97v8eAr4DqqfaxKevx0Ar8O+BR9yf5NYATlhrDzgdyhhT+Nz7fsaY6riOa4a/6N37HANstNa+e5HNMv2YpSWXE8fMGFPAGJPb/TgL0ADYlGqz74FH3Y9bAvOt+9MnJ3Olep+0Ga7PFTKUtbaXtba4tTYW1weU86217VJtlunHKy25nDhexphsxpgc5x4DDYHUV6759PUYcdVpM4AxZiKuqxPyG2P2Aq/h+kAHa+1IYCauT3G3AmeAx/wkV0ugizEmCTgLtM7ov8RutYCHgbXu908BegMlz8vmxDFLSy4njlkRYJwxJhzXPxjfWGt/MMb0A1ZYa7/H9Q/PeGPMVlwfXLfO4ExpzfWsMaYZkOTO1T4Tcl2QHxyvtORy4ngVAr5zn5dEAF9aa2cbYzpDxrwe9VV6EZEAFWhvoYiIiJsKXEQkQKnARUQClApcRCRAqcBFRAKUClxEJECpwEVEAtT/A24NOSdiKzbJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_x, training_y)\n",
    "plt.plot(training_x, hypothesis_theta(training_x, np.array([0.8132, 2.0512])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
