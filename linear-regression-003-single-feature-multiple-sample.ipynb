{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (0.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: pandas in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we used one input.  We call that a feature, or $x$.  And we also introduced the dummy feature $x_0$ that is always 1.  This completes our equation for the hypothesis, $h_\\theta$, which is based on all the features plus our parameters, $\\theta$:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sum_{j=1}^n(\\theta_jx_j)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\theta_j$ is our parameter at index $j$, which is our goal in linear regression\n",
    "- $x_j$ is our input feature at index $j$\n",
    "- $n$ is the number of features\n",
    "- $j$ is the index of the feature\n",
    "\n",
    "For example, when we have one feature, we would end up with this equation that looks like a basic linear equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1\n",
    "$$\n",
    "\n",
    "(remember $x_0$ is 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have multiple samples, for each sample, we will indicate the hypothesis of the sample as $h_\\theta(x)^{(i)}$, with a little $i$ on top, meaning the $i$th sample.  And our $i$th training $y$ would be indicated by $y^{(i)}$.\n",
    "\n",
    "The steps of linear regression are\n",
    "\n",
    "1. Pick some $\\theta_{0}$ and $\\theta_{1}$\n",
    "2. Calculate $h_{\\theta}(x)$ for every given sample of $x$\n",
    "3. Compare the calculated value with the actual value.  Calculate the overall error level (known as cost function $J$)\n",
    "   by using\n",
    "   $$\n",
    "     \\frac12\\sum_{i=1}^m(h_\\theta(x)^{(i)} - y^{(i)})^2\n",
    "   $$\n",
    "   where    \n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "4. Minimize the cost function $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the official definition but hard to comprehend all at once.\n",
    "- To make our lives easier, collectively we will call $\\theta_0$, $\\theta_1$, $\\theta_2$, ... as $\\theta$.  As we will see later, linear algebra allows us to treat arrays as single values.\n",
    "- Similarly, $x$ is all our input values (every feature and every sample)\n",
    "- $y$ is all our training target\n",
    "- $h_\\theta(x)$ is our hypothesis calcuated with $x$ and $\\theta$\n",
    "\n",
    "Now our simplified cost $J$ would look like:\n",
    "\n",
    "$$\n",
    "\\frac12(h_\\theta(x) - y)^2\n",
    "$$\n",
    "\n",
    "Kind of makes sense, without all the $\\sum$ and tiny symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will try a \"true\" linear training set.\n",
    "\n",
    "|x  |y  |\n",
    "|---|---|\n",
    "|1  |3  |\n",
    "|5  |11 |\n",
    "|2  |5  |\n",
    "\n",
    "At this point, we are still using one input feature so we are not overwhelmed when we try to debug our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = np.array([1, 5, 2])\n",
    "training_y = np.array([3, 11, 5])\n",
    "theta = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we just added the $x_0$ into our initial training data.  There was only one sample, so it was easy.  But with multiple samples, training data $x$ looks like \n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\\\\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "We want to pad it with a 1 so that the number of input variables (in our case, 1) equals the number of thetas (2).  The number of thetas is always 1 more than the number of input variables.  Don't get confused -- we have only one input variable for every training sample; however, we do have multiple training samples.  Refer to our notation guide from the previous example:\n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "\n",
    "In this example, $n$ is 1.\n",
    "\n",
    "Once we pad the 1, we will have this training data:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\ \\ \\ 1 \\\\\n",
    "1 \\ \\ \\ 5 \\\\\n",
    "1 \\ \\ \\ 2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Our hypothesis is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1\\theta_0 + 1\\theta_1 \\\\\n",
    "1\\theta_0 + 5\\theta_1 \\\\\n",
    "1\\theta_0 + 2\\theta_1 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 5.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want\n",
    "#    theta0 * x0 + theta1 * x1\n",
    "# (and remember x0 is set to 1 to simplify calculation)\n",
    "def padded_training_x(training_x):\n",
    "    # training_x looks like 1, 5, 2\n",
    "    size_of_sample = training_x.size\n",
    "    padding = np.ones(size_of_sample)    \n",
    "    padded = np.array([padding, training_x])\n",
    "    # but we want it in a different orientation\n",
    "    return padded.T\n",
    "padded_training_x(training_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hypothesis_theta(training_x, theta):\n",
    "    return padded_training_x(training_x) @ theta\n",
    "current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "current_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hypothesis for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_alpha = 0.01\n",
    "acceptable_cost = 0.01\n",
    "max_iterations = 100\n",
    "\n",
    "def cost(hypothesis, training_y):\n",
    "    return np.sum((hypothesis - training_y) ** 2) / 2\n",
    "current_cost = cost(current_hypothesis, training_y)\n",
    "current_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still one cost because cost is based on all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calclate our slope.  For completeness, here's the official equation:\n",
    "\n",
    "$$\n",
    "slope = \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19., -68.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slope(hypothesis, training_y, training_x):\n",
    "    # we have our padded training samples\n",
    "    # 1  1\n",
    "    # 1  5\n",
    "    # 1  2\n",
    "\n",
    "    # for each row, we want to multiply by the difference\n",
    "    # 1 x -3    1 x -3\n",
    "    # 1 x -11   5 x -11\n",
    "    # 1 x -15   2 x -15\n",
    "\n",
    "    # then we want to sum up by column\n",
    "    # 1 x -3 + 1 x - 11 + 1 x -15         1 x -3 + 5 x -11 + 2 x -15\n",
    "    \n",
    "    # looks awefully like\n",
    "    #\n",
    "    # | 1 1 1 |     | -3|\n",
    "    # | 1 5 2 |  X  |-11|\n",
    "    #               |-15|\n",
    "    return padded_training_x(training_x).T @ (hypothesis - training_y)\n",
    "current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "current_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the same equation as in previous examples but with multiple features and samples.  Suddenly it looks impossible to compute, or does it? (Remember, there is one slope for each feature.)\n",
    "\n",
    "And our gradient descent step becomes:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9, 6.8])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(theta, hypothesis, training_y, learning_rate_alpha):\n",
    "    return theta - learning_rate_alpha * slope(hypothesis, training_y, training_x)\n",
    "gradient_descent(theta, current_hypothesis, training_y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum(training_x, training_y):\n",
    "    max_iterations = 100\n",
    "    theta = np.array([0, 0])\n",
    "    iteration = 0\n",
    "    acceptable_slope = 0.00005\n",
    "    learning_rate_alpha = 0.01\n",
    "    current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "    current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "    current_cost = cost(hypothesis_theta(training_x, theta), training_y)\n",
    "    while abs(np.average(current_slope)) > acceptable_slope and iteration < max_iterations:\n",
    "        current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "        current_cost = cost(current_hypothesis, training_y)\n",
    "        current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "        theta = gradient_descent(theta, current_hypothesis, training_y, learning_rate_alpha)\n",
    "        print((theta, current_hypothesis, current_slope, current_cost))\n",
    "        iteration += 1\n",
    "    return (theta, current_hypothesis, current_slope, current_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through one round of gradient descent just to make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_round_0 = np.array([0, 0])\n",
    "#          x   hypothesis\n",
    "# (0)(1) + 0            0\n",
    "# (0)(5) + 0            0\n",
    "# (0)(2) + 0            0\n",
    "hypothesis_round_0 = hypothesis_theta(training_x, theta_round_0)\n",
    "hypothesis_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  difference**2\n",
    "# (0)(1) + 0            0  3          -3              9\n",
    "# (0)(5) + 0            0 11         -11            121\n",
    "# (0)(2) + 0            0  5          -5             25\n",
    "\n",
    "# total                                             155\n",
    "cost_round_0 = cost(hypothesis_round_0, training_y)\n",
    "cost_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19., -68.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  slope_for_theta_0 slope_for_theta_1\n",
    "# (0)(1) + 0            0  3          -3                 -3                -3\n",
    "# (0)(5) + 0            0 11         -11                -11               -55\n",
    "# (0)(2) + 0            0  5          -5                 -5               -10\n",
    "\n",
    "# total                                                 -19               -68\n",
    "\n",
    "slope_round_0 = slope(hypothesis_round_0, training_y, training_x)\n",
    "slope_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.19, 0.68]), array([0., 0., 0.]), array([-19., -68.]), 77.5)\n",
      "(array([0.3199, 1.1408]), array([0.87, 3.59, 1.55]), array([-12.99, -46.08]), 35.673750000000005)\n",
      "(array([0.409039, 1.452968]), array([1.4607, 6.0239, 2.6015]), array([ -8.9139, -31.2168]), 16.441908974999997)\n",
      "(array([0.47053039, 1.66435448]), array([1.862007, 7.673879, 3.314975]), array([ -6.149139, -21.138648]), 7.598709112657499)\n",
      "(array([0.51326612, 1.8074057 ]), array([2.13488487, 8.79230279, 3.79923935]), array([ -4.27357299, -14.30512248]), 3.5320886488925596)\n",
      "(array([0.54327568, 1.9041227 ]), array([2.32067182, 9.55029464, 4.12807753]), array([-3.000956 , -9.6716999]), 1.6616905919121403)\n",
      "(array([0.56464759, 1.96942384]), array([ 2.44739838, 10.0638892 ,  4.35152109]), array([-2.13719133, -6.53011345]), 0.8010984392836744)\n",
      "(array([0.58015426, 2.01342488]), array([ 2.53407143, 10.41176678,  4.50349527]), array([-1.55066651, -4.40010411]), 0.4048123470338611)\n",
      "(array([0.59167564, 2.04298507]), array([ 2.59357914, 10.64727865,  4.60700402]), array([-1.15213819, -2.95601955]), 0.22201805363042829)\n",
      "(array([0.60048657, 2.0627555 ]), array([ 2.63466072, 10.80660101,  4.67764579]), array([-0.88109248, -1.97704263]), 0.13739409869421815)\n",
      "(array([0.60745153, 2.07588993]), array([ 2.66324207, 10.91426407,  4.72599757]), array([-0.6964963 , -1.31344244]), 0.09791694426925235)\n",
      "(array([0.61315679, 2.08452683]), array([ 2.68334145, 10.98690116,  4.75923138]), array([-0.57052601, -0.86369001]), 0.07920687171372698)\n",
      "(array([0.61799994, 2.09011623]), array([ 2.69768361, 11.03579092,  4.78221044]), array([-0.48431503, -0.55894092]), 0.07005423985826334)\n",
      "(array([0.62225064, 2.09364137]), array([ 2.70811617, 11.06858111,  4.79823241]), array([-0.42507031, -0.35251344]), 0.06530484919157929)\n",
      "(array([0.62609181, 2.09576891]), array([ 2.71589201, 11.09045749,  4.80953338]), array([-0.38411712, -0.21275379]), 0.06258871996718074)\n",
      "(array([0.62964755, 2.09695089]), array([ 2.72186072, 11.10493635,  4.81762963]), array([-0.3555733 , -0.11819828]), 0.06081602465915498)\n",
      "(array([0.63300205, 2.09749382]), array([ 2.72659844, 11.114402  ,  4.82354933]), array([-0.33545024, -0.05429293]), 0.05948553620704203)\n",
      "(array([0.63621248, 2.09760551]), array([ 2.73049587, 11.12047115,  4.82798969]), array([-0.3210433 , -0.01116903]), 0.05836666100637261)\n",
      "(array([0.63931767, 2.09742686]), array([ 2.73381799, 11.12424003,  4.8314235 ]), array([-0.31051848,  0.01786514]), 0.057353241561189094)\n",
      "(array([0.64234399, 2.09705339]), array([ 2.73674452, 11.12645196,  4.83417138]), array([-0.30263214,  0.03734708]), 0.05639633671055326)\n",
      "(array([0.6453094 , 2.09654985]), array([ 2.73939737, 11.12761093,  4.83645076]), array([-0.29654094,  0.05035352]), 0.05547331477069584)\n",
      "(array([0.64822613, 2.09596014]), array([ 2.74185925, 11.12805866,  4.8384091 ]), array([-0.29167299,  0.05897074]), 0.05457364290875036)\n",
      "(array([0.65110253, 2.09531401]), array([ 2.74418627, 11.12802685,  4.84014642]), array([-0.28764046,  0.06461336]), 0.053692353238528806)\n",
      "(array([0.65394433, 2.09463161]), array([ 2.74641654, 11.12767259,  4.84173055]), array([-0.28418032,  0.06824059]), 0.05282703857900568)\n",
      "(array([0.65675548, 2.09392658]), array([ 2.74857594, 11.12710236,  4.84320755]), array([-0.28111415,  0.07050284]), 0.05197647110874393)\n",
      "(array([0.65953869, 2.09320817]), array([ 2.75068205, 11.12638836,  4.84460863]), array([-0.27832096,  0.07184112]), 0.05113996720838795)\n",
      "(array([0.66229587, 2.09248262]), array([ 2.75274685, 11.12557951,  4.84595502]), array([-0.27571862,  0.07255446]), 0.050317095391747106)\n",
      "(array([0.66502839, 2.09175417]), array([ 2.75477849, 11.12470898,  4.84726111]), array([-0.27325142,  0.07284561]), 0.04950754198528758)\n",
      "(array([0.6677372 , 2.09102564]), array([ 2.75678255, 11.12379921,  4.84853672]), array([-0.27088152,  0.07285204]), 0.04871104933889441)\n",
      "(array([0.67042303, 2.09029898]), array([ 2.75876285, 11.12286542,  4.84978849]), array([-0.26858324,  0.07266695]), 0.04792738738610589)\n",
      "(array([0.67308642, 2.08957544]), array([ 2.76072201, 11.12191791,  4.85102098]), array([-0.2663391 ,  0.07235352]), 0.047156340538501304)\n",
      "(array([0.6757278 , 2.08885589]), array([ 2.76266186, 11.12096362,  4.8522373 ]), array([-0.26413721,  0.07195459]), 0.04639770163101926)\n",
      "(array([0.67834749, 2.0881409 ]), array([ 2.76458369, 11.12000727,  4.85343958]), array([-0.26196946,  0.07149919]), 0.04565126910967122)\n",
      "(array([0.68094579, 2.08743083]), array([ 2.76648839, 11.119052  ,  4.8546293 ]), array([-0.25983031,  0.07100699]), 0.04491684571052215)\n",
      "(array([0.68352295, 2.08672592]), array([ 2.76837663, 11.11809996,  4.85580746]), array([-0.25771596,  0.07049132]), 0.04419423782479241)\n",
      "(array([0.68607919, 2.08602631]), array([ 2.77024887, 11.11715255,  4.85697479]), array([-0.25562379,  0.0699612 ]), 0.04348325517986677)\n",
      "(array([0.68861471, 2.08533208]), array([ 2.7721055 , 11.11621073,  4.85813181]), array([-0.25355197,  0.06942274]), 0.04278371066599525)\n",
      "(array([0.6911297 , 2.08464328]), array([ 2.77394679, 11.11527511,  4.85927887]), array([-0.25149923,  0.06888008]), 0.04209542023040337)\n",
      "(array([0.69362435, 2.08395992]), array([ 2.77577298, 11.1143461 ,  4.86041626]), array([-0.24946466,  0.06833599]), 0.041418202802823635)\n",
      "(array([0.69609883, 2.083282  ]), array([ 2.77758427, 11.11342394,  4.86154419]), array([-0.2474476 ,  0.06779237]), 0.04075188023588484)\n",
      "(array([0.6985533 , 2.08260949]), array([ 2.77938082, 11.1125088 ,  4.86266282]), array([-0.24544756,  0.06725047]), 0.0400962772527432)\n",
      "(array([0.70098794, 2.08194238]), array([ 2.78116279, 11.11160075,  4.86377228]), array([-0.24346417,  0.06671113]), 0.039451221398444786)\n",
      "(array([0.70340291, 2.08128063]), array([ 2.78293032, 11.11069984,  4.8648727 ]), array([-0.24149714,  0.06617492]), 0.03881654299339538)\n",
      "(array([0.70579838, 2.08062421]), array([ 2.78468354, 11.10980606,  4.86596417]), array([-0.23954622,  0.06564222]), 0.038192075088194284)\n",
      "(array([0.70817449, 2.07997308]), array([ 2.78642258, 11.10891942,  4.86704679]), array([-0.23761121,  0.06511325]), 0.037577653419471176)\n",
      "(array([0.71053141, 2.07932719]), array([ 2.78814756, 11.10803987,  4.86812064]), array([-0.23569193,  0.06458817]), 0.0369731163665652)\n",
      "(array([0.71286929, 2.07868652]), array([ 2.7898586 , 11.10716738,  4.8691858 ]), array([-0.23378823,  0.06406707]), 0.0363783049089591)\n",
      "(array([0.71518829, 2.07805102]), array([ 2.79155581, 11.1063019 ,  4.87024234]), array([-0.23189995,  0.06355001]), 0.03579306258442545)\n",
      "(array([0.71748856, 2.07742065]), array([ 2.79323931, 11.1054434 ,  4.87129034]), array([-0.23002695,  0.063037  ]), 0.03521723544785927)\n",
      "(array([0.71977025, 2.07679537]), array([ 2.79490921, 11.10459182,  4.87232986]), array([-0.2281691 ,  0.06252806]), 0.03465067203077876)\n",
      "(array([0.72203351, 2.07617514]), array([ 2.79656562, 11.10374711,  4.87336099]), array([-0.22632627,  0.06202317]), 0.0340932233014791)\n",
      "(array([0.7242785 , 2.07555992]), array([ 2.79820865, 11.10290922,  4.87438379]), array([-0.22449834,  0.06152232]), 0.03354474262583064)\n",
      "(array([0.72650535, 2.07494966]), array([ 2.79983841, 11.10207808,  4.87539833]), array([-0.22268517,  0.06102549]), 0.033005085728705925)\n",
      "(array([0.72871421, 2.07434434]), array([ 2.80145501, 11.10125366,  4.87640467]), array([-0.22088666,  0.06053266]), 0.03247411065602876)\n",
      "(array([0.73090524, 2.0737439 ]), array([ 2.80305855, 11.10043589,  4.87740289]), array([-0.21910267,  0.06004379]), 0.031951677737434604)\n",
      "(array([0.73307857, 2.07314831]), array([ 2.80464914, 11.09962473,  4.87839304]), array([-0.21733309,  0.05955887]), 0.031437649549528976)\n",
      "(array([0.73523435, 2.07255753]), array([ 2.80622688, 11.09882012,  4.87937519]), array([-0.21557781,  0.05907786]), 0.03093189087974216)\n",
      "(array([0.73737272, 2.07197152]), array([ 2.80779188, 11.098022  ,  4.88034941]), array([-0.2138367 ,  0.05860072]), 0.03043426869075852)\n",
      "(array([0.73949381, 2.07139025]), array([ 2.80934424, 11.09723033,  4.88131576]), array([-0.21210966,  0.05812744]), 0.029944652085524454)\n",
      "(array([0.74159778, 2.07081367]), array([ 2.81088406, 11.09644506,  4.88227431]), array([-0.21039657,  0.05765798]), 0.02946291227281517)\n",
      "(array([0.74368475, 2.07024175]), array([ 2.81241145, 11.09566613,  4.88322512]), array([-0.20869731,  0.05719231]), 0.028988922533357355)\n",
      "(array([0.74575487, 2.06967444]), array([ 2.8139265 , 11.09489348,  4.88416824]), array([-0.20701177,  0.0567304 ]), 0.028522558186495774)\n",
      "(array([0.74780827, 2.06911172]), array([ 2.81542931, 11.09412708,  4.88510375]), array([-0.20533985,  0.05627222]), 0.02806369655739726)\n",
      "(array([0.74984508, 2.06855354]), array([ 2.81691999, 11.09336687,  4.88603171]), array([-0.20368143,  0.05581775]), 0.027612216944781384)\n",
      "(array([0.75186545, 2.06799987]), array([ 2.81839863, 11.09261279,  4.88695217]), array([-0.20203641,  0.05536694]), 0.027168000589170793)\n",
      "(array([0.75386949, 2.06745068]), array([ 2.81986532, 11.09186481,  4.88786519]), array([-0.20040467,  0.05491977]), 0.026730930641651034)\n",
      "(array([0.75585736, 2.06690591]), array([ 2.82132017, 11.09112287,  4.88877084]), array([-0.19878612,  0.05447621]), 0.026300892133136335)\n",
      "(array([0.75782916, 2.06636555]), array([ 2.82276327, 11.09038692,  4.88966918]), array([-0.19718063,  0.05403624]), 0.025877771944125067)\n",
      "(array([0.75978504, 2.06582955]), array([ 2.82419471, 11.08965692,  4.89056026]), array([-0.19558811,  0.05359982]), 0.025461458774945826)\n",
      "(array([0.76172513, 2.06529788]), array([ 2.8256146 , 11.08893281,  4.89144415]), array([-0.19400845,  0.05316692]), 0.02505184311647978)\n",
      "(array([0.76364954, 2.06477051]), array([ 2.82702301, 11.08821454,  4.89232089]), array([-0.19244155,  0.05273752]), 0.024648817221355503)\n",
      "(array([0.76555842, 2.06424739]), array([ 2.82842005, 11.08750208,  4.89319056]), array([-0.19088731,  0.05231159]), 0.02425227507560593)\n",
      "(array([0.76745187, 2.0637285 ]), array([ 2.82980581, 11.08679538,  4.8940532 ]), array([-0.18934561,  0.0518891 ]), 0.02386211237078213)\n",
      "(array([0.76933004, 2.0632138 ]), array([ 2.83118037, 11.08609438,  4.89490887]), array([-0.18781637,  0.05147002]), 0.023478226476515752)\n",
      "(array([0.77119303, 2.06270326]), array([ 2.83254384, 11.08539904,  4.89575764]), array([-0.18629948,  0.05105432]), 0.02310051641352218)\n",
      "(array([0.77304098, 2.06219684]), array([ 2.83389629, 11.08470932,  4.89659955]), array([-0.18479484,  0.05064198]), 0.022728882827039068)\n",
      "(array([0.774874  , 2.06169451]), array([ 2.83523782, 11.08402517,  4.89743466]), array([-0.18330236,  0.05023298]), 0.02236322796069035)\n",
      "(array([0.77669222, 2.06119624]), array([ 2.83656851, 11.08334654,  4.89826302]), array([-0.18182193,  0.04982727]), 0.022003455630773633)\n",
      "(array([0.77849576, 2.06070199]), array([ 2.83788846, 11.0826734 ,  4.89908469]), array([-0.18035345,  0.04942484]), 0.021649471200957806)\n",
      "(array([0.78028472, 2.06021173]), array([ 2.83919774, 11.08200569,  4.89989973]), array([-0.17889683,  0.04902567]), 0.021301181557390844)\n",
      "(array([0.78205924, 2.05972543]), array([ 2.84049646, 11.08134338,  4.90070819]), array([-0.17745198,  0.04862971]), 0.020958495084205827)\n",
      "(array([0.78381943, 2.05924306]), array([ 2.84178468, 11.08068641,  4.90151011]), array([-0.1760188 ,  0.04823696]), 0.020621321639421902)\n",
      "(array([0.7855654 , 2.05876459]), array([ 2.8430625 , 11.08003475,  4.90230556]), array([-0.17459719,  0.04784737]), 0.020289572531233282)\n",
      "(array([0.78729728, 2.05828998]), array([ 2.84432999, 11.07938835,  4.90309458]), array([-0.17318707,  0.04746094]), 0.019963160494678896)\n",
      "(array([0.78901516, 2.0578192 ]), array([ 2.84558726, 11.07874718,  4.90387724]), array([-0.17178833,  0.04707762]), 0.019641999668688327)\n",
      "(array([0.79071917, 2.05735223]), array([ 2.84683436, 11.07811118,  4.90465357]), array([-0.17040089,  0.0466974 ]), 0.019326005573495418)\n",
      "(array([0.79240941, 2.05688903]), array([ 2.8480714 , 11.07748032,  4.90542363]), array([-0.16902465,  0.04632025]), 0.019015095088417696)\n",
      "(array([0.79408601, 2.05642957]), array([ 2.84929844, 11.07685455,  4.90618747]), array([-0.16765953,  0.04594615]), 0.018709186429990896)\n",
      "(array([0.79574906, 2.05597382]), array([ 2.85051558, 11.07623384,  4.90694514]), array([-0.16630544,  0.04557507]), 0.01840819913045627)\n",
      "(array([0.79739869, 2.05552175]), array([ 2.85172288, 11.07561814,  4.9076967 ]), array([-0.16496228,  0.04520698]), 0.018112054016594506)\n",
      "(array([0.79903499, 2.05507333]), array([ 2.85292043, 11.07500742,  4.90844218]), array([-0.16362997,  0.04484187]), 0.017820673188898864)\n",
      "(array([0.80065807, 2.05462853]), array([ 2.85410831, 11.07440162,  4.90918164]), array([-0.16230842,  0.04447971]), 0.017533980001085164)\n",
      "(array([0.80226805, 2.05418733]), array([ 2.8552866 , 11.07380072,  4.90991513]), array([-0.16099755,  0.04412047]), 0.01725189903992901)\n",
      "(array([0.80386502, 2.05374968]), array([ 2.85645537, 11.07320467,  4.9106427 ]), array([-0.15969726,  0.04376413]), 0.01697435610542992)\n",
      "(array([0.80544909, 2.05331558]), array([ 2.8576147 , 11.07261344,  4.91136439]), array([-0.15840747,  0.04341067]), 0.016701278191292288)\n",
      "(array([0.80702037, 2.05288498]), array([ 2.85876467, 11.07202698,  4.91208025]), array([-0.1571281 ,  0.04306007]), 0.016432593465722664)\n",
      "(array([0.80857896, 2.05245785]), array([ 2.85990535, 11.07144526,  4.91279033]), array([-0.15585906,  0.0427123 ]), 0.01616823125253359)\n",
      "(array([0.81012497, 2.05203418]), array([ 2.86103682, 11.07086823,  4.91349467]), array([-0.15460028,  0.04236733]), 0.015908122012553483)\n",
      "(array([0.81165848, 2.05161393]), array([ 2.86215915, 11.07029587,  4.91419333]), array([-0.15335165,  0.04202515]), 0.01565219732533402)\n",
      "(array([0.81317962, 2.05119707]), array([ 2.86327241, 11.06972813,  4.91488634]), array([-0.15211312,  0.04168574]), 0.015400389871152885)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.81317962, 2.05119707]),\n",
       " array([ 2.86327241, 11.06972813,  4.91488634]),\n",
       " array([-0.15211312,  0.04168574]),\n",
       " 0.015400389871152885)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum(training_x, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our result with the correct $\\theta_0 = 1$ and $\\theta_1 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17df154c4c0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_x, training_y)\n",
    "plt.plot(training_x, hypothesis_theta(training_x, np.array([0.8131, 2.0512])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
