{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (3.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: pandas in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we have done training samples that were just constants without any input.  Now we are going to try a sample with an input and output.\n",
    "\n",
    "|x |y |\n",
    "|--|--|\n",
    "|2 |3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major difference between this and the previous examples is that we can no longer directly guess the hypothesis, since the hypothesis depends on our input, $x$.\n",
    "\n",
    "In linear regression, we are creating a linear function to calculate value $f(x)$ for a given $x$.  To find $f(x)$, we are given sample values of $x$ and $y$.  We want our $f(x)$ to be very close to $y$ for every value of $x$.  The hope is that for any other values of $x$ not in the sample, $f(x)$ would predict the correct value of $y$.\n",
    "\n",
    "The equation is just like any linear equation, in the form of\n",
    "\n",
    "$$\n",
    "f(x) = mx + b\n",
    "$$\n",
    "\n",
    "But in linear-regression-speak, we write\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "$$\n",
    "\n",
    "$h_\\theta(x)$ is our hypothesis, which is now based on our guess of input $x$, parameter $\\theta_0$, and parameter $\\theta_1$.\n",
    "\n",
    "(If we had more than one input, we would need more $\\theta$s for each additional input)\n",
    "\n",
    "Our job is to guess the correct $\\theta_0$ and $\\theta_1$.  There are two values to guess even though there is only one input.  That makes sense because even when we had no input, we still had to guess one value.  The number of values to guess is always one more than the number of inputs.\n",
    "\n",
    "**But it's confusing to have to guess two values given only one input, so we are going to make our lives easier by inserting a dummy input of 1, so that our sample looks like this:**\n",
    "\n",
    "|x0 |x1 |y  |\n",
    "|---|---|---|\n",
    "|1  |2  |3  |\n",
    "\n",
    "That makes our function look more uniform:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of linear regression are\n",
    "\n",
    "1. Pick some $\\theta_{0}$ and $\\theta_{1}$\n",
    "2. Calculate $h_{\\theta}(x)$ for every given sample of $x_0$ and $x_1$ (remember $x_0$ is always 1)\n",
    "3. Compare the calculated value with the actual value.  Calculate the overall error level (known as cost function $J$)\n",
    "   by using\n",
    "   $$\n",
    "     J = \\frac 1 2 (h_\\theta(x) - y)^2\n",
    "   $$\n",
    "   where\n",
    "   - $x$ is the collective term for $x_0$ and $x_1$ (and possibly more if we had more inputs)\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "4. Minimize the cost function $J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = np.array([1, 2])\n",
    "training_y = 3\n",
    "theta = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a hypothesis function that would calculate our hypothesis based on $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hypothesis_theta(training_x, theta):\n",
    "    # @ is the dot product, meaning multiplying each corresponding element in the array, and sum up the products\n",
    "    return training_x @ theta\n",
    "\n",
    "# this is 1 * 0 + 2 * 0\n",
    "current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "current_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost(hypothesis, training_y):\n",
    "    return (hypothesis - training_y) ** 2 / 2\n",
    "current_cost = cost(current_hypothesis, training_y)\n",
    "current_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope is tricky.  There are actually two slopes (in this example), one for $\\theta_0$ and another for $\\theta_1$.  The derivation in this one is a bit more complicated, so we won't talk about it in detail.\n",
    "\n",
    "For $\\theta_0$, the slope is $(h_\\theta(x) - y)x_0$\n",
    "\n",
    "For $\\theta_1$, the slope is $(h_\\theta(x) - y)x_1$\n",
    "\n",
    "Kind of make sense if we look at the slope for $\\theta_0$: since $x_0$ is always 1, the slope is $(h_\\theta(x) - y)$, which is what we had in our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slope(hypothesis, training_y, training_x):\n",
    "    return (hypothesis - training_y) * training_x\n",
    "\n",
    "# theta0 = (0 - 3) * 1\n",
    "# theta1 = (0 - 3) * 2\n",
    "slope(current_hypothesis, training_y, training_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3, 0.6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(theta, hypothesis, training_y, learning_rate_alpha):\n",
    "    return theta - learning_rate_alpha * slope(hypothesis, training_y, training_x)\n",
    "gradient_descent(theta, current_hypothesis, training_y, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see our $\\theta$ slowly increasing.  Our linear regression is now complicated enough that we can't immediately tell if our result is correct.  But let's go ahead and complete the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum():\n",
    "    theta = np.array([0, 0])\n",
    "    iteration = 0\n",
    "    acceptable_slope = 0.00005\n",
    "    learning_rate_alpha = 0.1\n",
    "    current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "    current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "    current_cost = 0\n",
    "    while abs(np.average(current_slope)) > acceptable_slope and iteration < 100:\n",
    "        current_hypothesis = hypothesis_theta(training_x, theta)\n",
    "        current_cost = cost(current_hypothesis, training_y)\n",
    "        current_slope = slope(current_hypothesis, training_y, training_x)\n",
    "        theta = gradient_descent(theta, current_hypothesis, training_y, learning_rate_alpha)\n",
    "        print((theta, current_hypothesis, current_slope, current_cost))\n",
    "        iteration += 1\n",
    "    return (theta, current_hypothesis, current_slope, current_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.3, 0.6]), 0, array([-3, -6]), 4.5)\n",
      "(array([0.45, 0.9 ]), 1.5000000000000002, array([-1.5, -3. ]), 1.1249999999999998)\n",
      "(array([0.525, 1.05 ]), 2.2500000000000004, array([-0.75, -1.5 ]), 0.28124999999999967)\n",
      "(array([0.5625, 1.125 ]), 2.625, array([-0.375, -0.75 ]), 0.0703125)\n",
      "(array([0.58125, 1.1625 ]), 2.8125, array([-0.1875, -0.375 ]), 0.017578125)\n",
      "(array([0.590625, 1.18125 ]), 2.90625, array([-0.09375, -0.1875 ]), 0.00439453125)\n",
      "(array([0.5953125, 1.190625 ]), 2.9531250000000004, array([-0.046875, -0.09375 ]), 0.0010986328124999792)\n",
      "(array([0.59765625, 1.1953125 ]), 2.9765625, array([-0.0234375, -0.046875 ]), 0.000274658203125)\n",
      "(array([0.59882813, 1.19765625]), 2.98828125, array([-0.01171875, -0.0234375 ]), 6.866455078125e-05)\n",
      "(array([0.59941406, 1.19882813]), 2.994140625, array([-0.00585938, -0.01171875]), 1.71661376953125e-05)\n",
      "(array([0.59970703, 1.19941406]), 2.9970703125000004, array([-0.00292969, -0.00585937]), 4.291534423826824e-06)\n",
      "(array([0.59985352, 1.19970703]), 2.99853515625, array([-0.00146484, -0.00292969]), 1.0728836059570312e-06)\n",
      "(array([0.59992676, 1.19985352]), 2.999267578125, array([-0.00073242, -0.00146484]), 2.682209014892578e-07)\n",
      "(array([0.59996338, 1.19992676]), 2.9996337890625, array([-0.00036621, -0.00073242]), 6.705522537231445e-08)\n",
      "(array([0.59998169, 1.19996338]), 2.9998168945312504, array([-0.00018311, -0.00036621]), 1.6763806342997298e-08)\n",
      "(array([0.59999084, 1.19998169]), 2.999908447265625, array([-9.15527344e-05, -1.83105469e-04]), 4.190951585769653e-09)\n",
      "(array([0.59999542, 1.19999084]), 2.9999542236328125, array([-4.57763672e-05, -9.15527344e-05]), 1.0477378964424133e-09)\n",
      "(array([0.59999771, 1.19999542]), 2.9999771118164062, array([-2.28881836e-05, -4.57763672e-05]), 2.6193447411060333e-10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.59999771, 1.19999542]),\n",
       " 2.9999771118164062,\n",
       " array([-2.28881836e-05, -4.57763672e-05]),\n",
       " 2.6193447411060333e-10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go verify to see if the result is correct.  It may not be what we were expecting, but since we only have one data point, there are multiple possible results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
