{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\jojo\\jupyter\\lib\\site-packages (3.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jojo\\jupyter\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (8.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jojo\\jupyter\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jojo\\jupyter\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jojo\\jupyter\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief description of linear regression.  In linear regression, we are given a set of input values, such as square footage of a house and number of bedrooms.  And we are asked to predict an output, such as the price of the house.  We are given training samples of houses for which we know the input and the output, in this case, square footage, number of bedrooms, and the price.  We are asked to create a function that would take the input and calculate the output, so that later on, given just the square footage and the number of bedrooms, we can predict the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the data would look like something below, and we are trying to find correlation between the x axis and the y axis.  In this example, we only have one input x, so it is easier to visualize.  In reality, we may have multiple x's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1021a8c8>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([1,2,3,4,5,6,7,8,9,10], [4,4,7,5,6,6,10,12,11,13], 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we are going to make things simpler, so we can more intuitively understand the entire process.  We are going to have one sample, one output, and zero input.  We are going to set out training data to be just the number 5.  Let's call it $y$.\n",
    "\n",
    "And we will arrive at that number through the formalities of linear regression.  Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with a guess.  Let's say we guess the number is 0.  Let's call our guess our hypothesis, $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to know the difference between our guess and the actual training sample.  And since we want to know the difference whether it is positive or negative, we will square it.  Think of it as the same as calculating standard deviation.  This difference is called the cost.  Let's also divide the square by 2.  We'll see why later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = (hypothesis - training_y) ** 2 / 2\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our cost is 25.  Our goal is to find the minumum cost, which would indicate that we are being closest the actual training output, $y$.  Before we go about doing that, let's take a look at the graph of all our possible $h$ values and their costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_hypotheses = np.arange(0, 10, 1)\n",
    "possible_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0a00ca356e88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpossible_costs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpossible_hypotheses\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpossible_costs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "possible_costs = (possible_hypotheses - y) ** 2 / 2\n",
    "possible_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(possible_hypotheses, possible_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, if we set our $h$ to be 5, we would be at the lowest cost.  That makes sense because we know our $y$ is 5.  But how can we get there from our current $h$ of 0?  Well, we can take a look at the slope.\n",
    "\n",
    "If our current slope is negative, we need to increase $h$.\n",
    "\n",
    "If our current slope is positive, we need to decrease $h$.\n",
    "\n",
    "And how do we find the slope?  It's the derivative of the cost function.  The cost function is\n",
    "\n",
    "$$\n",
    "\\frac 1 2(h - y)^2\n",
    "$$\n",
    "\n",
    "So the derivative would be\n",
    "\n",
    "$$\n",
    "h - y\n",
    "$$\n",
    "\n",
    "Now we see why we divided by 2 earlier, but it doesn't really matter because we are going to scale this value when we increase or decrease our guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right now, with our guess of 0, our slope is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope = hypothesis - training_y\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our slope is negative, we are going to increase our guess.  In other words, we are going to subtract the slope from our current guess.  Also, we are going to multiply the slope by a factor, called the learning rate $\\alpha$, so that we don't adjust our guess by too much at a time.  Let's say our learning rate $\\alpha$ is 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_alpha = 0.1\n",
    "hypothesis = hypothesis - learning_rate_alpha * slope\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have successfully increased our guess by 0.5.  Now we repeat the process until our slope is near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope = hypothesis - training_y\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = hypothesis - learning_rate_alpha * slope\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope = hypothesis - training_y\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.355"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = hypothesis - learning_rate_alpha * slope\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum():\n",
    "    hypothesis = 0\n",
    "    iteration = 0\n",
    "    acceptable_slope = 0.05\n",
    "    learning_rate_alpha = 0.1\n",
    "    slope = hypothesis - training_y\n",
    "    cost = 0\n",
    "    while abs(slope) > acceptable_slope and iteration < 100:\n",
    "        cost = (hypothesis - training_y) ** 2 / 2\n",
    "        slope = hypothesis - training_y\n",
    "        hypothesis = hypothesis - learning_rate_alpha * slope\n",
    "        print((hypothesis, slope, cost))\n",
    "        iteration += 1\n",
    "    return (hypothesis, slope, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, -5, 12.5)\n",
      "(0.95, -4.5, 10.125)\n",
      "(1.355, -4.05, 8.20125)\n",
      "(1.7195, -3.645, 6.6430125)\n",
      "(2.04755, -3.2805, 5.380840125)\n",
      "(2.342795, -2.95245, 4.358480501249999)\n",
      "(2.6085155, -2.657205, 3.5303692060124994)\n",
      "(2.8476639500000003, -2.3914845, 2.8595990568701244)\n",
      "(3.062897555, -2.1523360499999997, 2.3162752360648007)\n",
      "(3.2566077995000002, -1.9371024449999998, 1.8761829412124886)\n",
      "(3.43094701955, -1.7433922004999998, 1.5197081823821157)\n",
      "(3.587852317595, -1.56905298045, 1.230963627729514)\n",
      "(3.7290670858354997, -1.412147682405, 0.9970805384609065)\n",
      "(3.8561603772519497, -1.2709329141645003, 0.8076352361533345)\n",
      "(3.9705443395267546, -1.1438396227480503, 0.654184541284201)\n",
      "(4.073489905574079, -1.0294556604732454, 0.529889478440203)\n",
      "(4.166140915016671, -0.9265100944259208, 0.4292104775365643)\n",
      "(4.249526823515004, -0.8338590849833292, 0.3476604868046175)\n",
      "(4.324574141163504, -0.7504731764849959, 0.2816049943117399)\n",
      "(4.392116727047154, -0.6754258588364959, 0.22810004539250903)\n",
      "(4.452905054342438, -0.6078832729528463, 0.18476103676793232)\n",
      "(4.507614548908195, -0.5470949456575616, 0.1496564397820251)\n",
      "(4.5568530940173755, -0.4923854510918053, 0.1212217162234403)\n",
      "(4.601167784615638, -0.4431469059826245, 0.09818959014098652)\n",
      "(4.641051006154074, -0.3988322153843624, 0.07953356801419922)\n",
      "(4.676945905538666, -0.35894899384592627, 0.06442219009150141)\n",
      "(4.709251314984799, -0.3230540944613338, 0.0521819739741162)\n",
      "(4.738326183486319, -0.2907486850152008, 0.04226739891903422)\n",
      "(4.764493565137688, -0.2616738165136807, 0.03423659312441772)\n",
      "(4.788044208623919, -0.23550643486231237, 0.02773164043077829)\n",
      "(4.809239787761527, -0.21195579137608078, 0.02246262874893034)\n",
      "(4.828315808985375, -0.19076021223847306, 0.018194729286633644)\n",
      "(4.845484228086837, -0.1716841910146254, 0.01473773072217319)\n",
      "(4.860935805278154, -0.15451577191316268, 0.011937561884960257)\n",
      "(4.8748422247503385, -0.13906419472184606, 0.009669425126817758)\n",
      "(4.887358002275304, -0.12515777524966154, 0.007832234352722395)\n",
      "(4.898622202047774, -0.11264199772469574, 0.00634410982570518)\n",
      "(4.908759981842996, -0.10137779795222635, 0.005138728958821214)\n",
      "(4.9178839836586965, -0.09124001815700389, 0.004162370456645199)\n",
      "(4.926095585292827, -0.0821160163413035, 0.0033715200698826117)\n",
      "(4.9334860267635445, -0.07390441470717324, 0.002730931256604922)\n",
      "(4.94013742408719, -0.06651397323645547, 0.0022120543178499575)\n",
      "(4.946123681678471, -0.059862575912809746, 0.0017917639974584548)\n",
      "(4.951511313510624, -0.05387631832152895, 0.001451328837941358)\n",
      "(4.956360182159561, -0.04848868648937632, 0.0011755763587325129)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.956360182159561, -0.04848868648937632, 0.0011755763587325129)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how we get to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, do the same thing with a staring guess of 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
