{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\jojo\\stuff\\notebooks\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'd:\\jojo\\stuff\\notebooks\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we are creating a linear function to calculate value $f(x)$ for a given $x$.  To find $f(x)$, we are given sample values of $x$ and $y$.  We want our $f(x)$ to be very close to $y$ for every value of $x$.  The hope is that for any other values of $x$ not in the sample, $f(x)$ would predict the correct value of $y$.\n",
    "\n",
    "The equation is just like any linear equation, in the form of\n",
    "\n",
    "$$\n",
    "f(x) = mx + b\n",
    "$$\n",
    "\n",
    "But in linear-regression-speak, we write\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "$$\n",
    "\n",
    "(this is not complete as we will see later, but it is easier to understand for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of linear regression are\n",
    "\n",
    "1. Pick some $\\theta_{0}$ and $\\theta_{1}$\n",
    "2. Calculate $h_{\\theta}(x)$ for every given sample of $x$\n",
    "3. Compare the calculated value with the actual value.  Calculate the overall error level (known as cost function $J$)\n",
    "   by using\n",
    "   $$\n",
    "     \\frac12\\sum_{i=1}^m(h_\\theta(x)^{(i)} - y^{(i)})^2\n",
    "   $$\n",
    "   where    \n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "4. Minimize the cost function $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will try a \"true\" linear training set.\n",
    "\n",
    "|x  |y  |\n",
    "|---|---|\n",
    "|1  |3  |\n",
    "|5  |11 |\n",
    "|2  |5  |\n",
    "\n",
    "At this point, we are still using a single value of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we are creating a linear function to calculate value $f(x)$ for a given $x$.  To find $f(x)$, we are given sample values of $x$ and $y$.  We want our $f(x)$ to be very close to $y$ for every value of $x$.  The hope is that for any other values of $x$ not in the sample, $f(x)$ would predict the correct value of $y$.\n",
    "\n",
    "The equation is just like any linear equation, in the form of\n",
    "\n",
    "$$\n",
    "f(x) = mx + b\n",
    "$$\n",
    "\n",
    "But in linear-regression-speak, we write\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "$$\n",
    "\n",
    "(this is not complete as we will see later, but it is easier to understand for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = np.array([1, 5, 2])\n",
    "training_y = np.array([3, 11, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See previous notebook for a simpler example.  For our current example, however, we now have $\\theta_0$ and $\\theta_1$, so we need to calculate our hypothesis differently.\n",
    "\n",
    "Training data looks like \n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\\\\n",
    "5 \\\\\n",
    "2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "We want to pad it with a 1 so that the number of input variables (in our case, 1) equals the number of thetas (2).  The number of thetas is always 1 more than the number of input variables.  Don't get confused -- we have only one input variable for every training sample; however, we do have multiple training samples.  Refer to our notation guide from the previous example:\n",
    "   - $m$ is the number of training samples\n",
    "   - $y$ is the actual value of training sample for a given $x$\n",
    "   - $i$ is the index of the training sample\n",
    "   - $x^{(i)}$ is the $x$ of the the training sample of index $i$\n",
    "   - $j$ is the index of the input variable\n",
    "   - $n$ is the number of input variabls per training sample\n",
    "\n",
    "In this example, $n$ is 1.\n",
    "\n",
    "Once we pad the 1, we will have this training data:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1 \\ \\ \\ 1 \\\\\n",
    "1 \\ \\ \\ 5 \\\\\n",
    "1 \\ \\ \\ 2 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Our hypothesis is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1\\theta_0 + 1\\theta_1 \\\\\n",
    "1\\theta_0 + 5\\theta_1 \\\\\n",
    "1\\theta_0 + 2\\theta_1 \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want\n",
    "#    theta0 * x0 + theta1 * x1\n",
    "# (and remember x0 is set to 1 to simplify calculation)\n",
    "def padded_training_x(training_x):\n",
    "    # training_x looks like 1, 5, 2\n",
    "    size_of_sample = training_x.size\n",
    "    padding = np.ones(size_of_sample)    \n",
    "    padded = np.array([padding, training_x])\n",
    "    # but we want it in a different orientation\n",
    "    return padded.T\n",
    "    \n",
    "def hypothesis_theta(training_x, theta):\n",
    "    return padded_training_x(training_x) @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will repeat some of the functions from previous example (with slight modifications):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we start from $\\theta_0 = 0$, how do we get to 5?\n",
    "\n",
    "To understand this, we can look at the instantaneous slope (derivative) of the cost at $\\theta_0$.  If the slope is negative, we need to increase $\\theta_0$, and if the slope is positive, we need to increase $\\theta_0$.  Not only that, the amount to increase is proportional to the slope.  The proportion, in our case, is the learning rate $\\alpha$.\n",
    "\n",
    "We can get closer to the correct $\\theta_0$ by the gradient descent formula\n",
    "\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_0$ is 1 (so that $\\theta_0x$ = $\\theta_0$)\n",
    "- $\\alpha$ is the learning rate so we don't overshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_alpha = 0.01\n",
    "acceptable_cost = 0.01\n",
    "max_iterations = 100\n",
    "\n",
    "def cost(hypothesis, training_y):\n",
    "    return np.sum((hypothesis - training_y) ** 2) / 2\n",
    "\n",
    "def slope(hypothesis, training_y, training_x):\n",
    "    # we have our padded training samples\n",
    "    # 1  1\n",
    "    # 1  5\n",
    "    # 1  2\n",
    "\n",
    "    # for each row, we want to multiply by the difference\n",
    "    # 1 x -3    1 x -3\n",
    "    # 1 x -11   5 x -11\n",
    "    # 1 x -15   2 x -15\n",
    "\n",
    "    # then we want to sum up by column\n",
    "    # 1 x -3 + 1 x - 11 + 1 x -15         1 x -3 + 5 x -11 + 2 x -15\n",
    "    \n",
    "    # looks awefully like\n",
    "    #\n",
    "    # | 1 1 1 |     | -3|\n",
    "    # | 1 5 2 |  X  |-11|\n",
    "    #               |-15|\n",
    "    return padded_training_x(training_x).T @ (hypothesis - training_y)\n",
    "\n",
    "def gradient_descent(theta, training_y):\n",
    "    hypothesis = hypothesis_theta(training_x, theta)\n",
    "    return theta - learning_rate_alpha * slope(hypothesis_theta(training_x, theta), training_y, training_x)\n",
    "\n",
    "def find_minimum(training_x, training_y):\n",
    "    theta = np.array([0, 0])\n",
    "    iteration = 0\n",
    "    current_cost = cost(hypothesis_theta(training_x, theta), training_y)\n",
    "    while current_cost > acceptable_cost and iteration < max_iterations:\n",
    "        theta = gradient_descent(theta, training_y)\n",
    "        current_cost = cost(hypothesis_theta(training_x, theta), training_y)\n",
    "        print((theta, current_cost))\n",
    "        iteration += 1\n",
    "    return (theta, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through one round of gradient descent just to make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_round_0 = np.array([0, 0])\n",
    "#          x   hypothesis\n",
    "# (0)(1) + 0            0\n",
    "# (0)(5) + 0            0\n",
    "# (0)(2) + 0            0\n",
    "hypothesis_round_0 = hypothesis_theta(training_x, theta_round_0)\n",
    "hypothesis_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  difference**2\n",
    "# (0)(1) + 0            0  3          -3              9\n",
    "# (0)(5) + 0            0 11         -11            121\n",
    "# (0)(2) + 0            0  5          -5             25\n",
    "\n",
    "# total                                             155\n",
    "cost_round_0 = cost(hypothesis_round_0, training_y)\n",
    "cost_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19., -68.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          x   hypothesis  y  difference  slope_for_theta_0 slope_for_theta_1\n",
    "# (0)(1) + 0            0  3          -3                 -3                -3\n",
    "# (0)(5) + 0            0 11         -11                -11               -55\n",
    "# (0)(2) + 0            0  5          -5                 -5               -10\n",
    "\n",
    "# total                                                 -19               -68\n",
    "\n",
    "slope_round_0 = slope(hypothesis_round_0, training_y, training_x)\n",
    "slope_round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.19, 0.68]), 35.673750000000005)\n",
      "(array([0.3199, 1.1408]), 16.441908974999997)\n",
      "(array([0.409039, 1.452968]), 7.598709112657499)\n",
      "(array([0.47053039, 1.66435448]), 3.5320886488925596)\n",
      "(array([0.51326612, 1.8074057 ]), 1.6616905919121403)\n",
      "(array([0.54327568, 1.9041227 ]), 0.8010984392836744)\n",
      "(array([0.56464759, 1.96942384]), 0.4048123470338611)\n",
      "(array([0.58015426, 2.01342488]), 0.22201805363042829)\n",
      "(array([0.59167564, 2.04298507]), 0.13739409869421815)\n",
      "(array([0.60048657, 2.0627555 ]), 0.09791694426925235)\n",
      "(array([0.60745153, 2.07588993]), 0.07920687171372698)\n",
      "(array([0.61315679, 2.08452683]), 0.07005423985826334)\n",
      "(array([0.61799994, 2.09011623]), 0.06530484919157929)\n",
      "(array([0.62225064, 2.09364137]), 0.06258871996718074)\n",
      "(array([0.62609181, 2.09576891]), 0.06081602465915498)\n",
      "(array([0.62964755, 2.09695089]), 0.05948553620704203)\n",
      "(array([0.63300205, 2.09749382]), 0.05836666100637261)\n",
      "(array([0.63621248, 2.09760551]), 0.057353241561189094)\n",
      "(array([0.63931767, 2.09742686]), 0.05639633671055326)\n",
      "(array([0.64234399, 2.09705339]), 0.05547331477069584)\n",
      "(array([0.6453094 , 2.09654985]), 0.05457364290875036)\n",
      "(array([0.64822613, 2.09596014]), 0.053692353238528806)\n",
      "(array([0.65110253, 2.09531401]), 0.05282703857900568)\n",
      "(array([0.65394433, 2.09463161]), 0.05197647110874393)\n",
      "(array([0.65675548, 2.09392658]), 0.05113996720838795)\n",
      "(array([0.65953869, 2.09320817]), 0.050317095391747106)\n",
      "(array([0.66229587, 2.09248262]), 0.04950754198528758)\n",
      "(array([0.66502839, 2.09175417]), 0.04871104933889441)\n",
      "(array([0.6677372 , 2.09102564]), 0.04792738738610589)\n",
      "(array([0.67042303, 2.09029898]), 0.047156340538501304)\n",
      "(array([0.67308642, 2.08957544]), 0.04639770163101926)\n",
      "(array([0.6757278 , 2.08885589]), 0.04565126910967122)\n",
      "(array([0.67834749, 2.0881409 ]), 0.04491684571052215)\n",
      "(array([0.68094579, 2.08743083]), 0.04419423782479241)\n",
      "(array([0.68352295, 2.08672592]), 0.04348325517986677)\n",
      "(array([0.68607919, 2.08602631]), 0.04278371066599525)\n",
      "(array([0.68861471, 2.08533208]), 0.04209542023040337)\n",
      "(array([0.6911297 , 2.08464328]), 0.041418202802823635)\n",
      "(array([0.69362435, 2.08395992]), 0.04075188023588484)\n",
      "(array([0.69609883, 2.083282  ]), 0.0400962772527432)\n",
      "(array([0.6985533 , 2.08260949]), 0.039451221398444786)\n",
      "(array([0.70098794, 2.08194238]), 0.03881654299339538)\n",
      "(array([0.70340291, 2.08128063]), 0.038192075088194284)\n",
      "(array([0.70579838, 2.08062421]), 0.037577653419471176)\n",
      "(array([0.70817449, 2.07997308]), 0.0369731163665652)\n",
      "(array([0.71053141, 2.07932719]), 0.0363783049089591)\n",
      "(array([0.71286929, 2.07868652]), 0.03579306258442545)\n",
      "(array([0.71518829, 2.07805102]), 0.03521723544785927)\n",
      "(array([0.71748856, 2.07742065]), 0.03465067203077876)\n",
      "(array([0.71977025, 2.07679537]), 0.0340932233014791)\n",
      "(array([0.72203351, 2.07617514]), 0.03354474262583064)\n",
      "(array([0.7242785 , 2.07555992]), 0.033005085728705925)\n",
      "(array([0.72650535, 2.07494966]), 0.03247411065602876)\n",
      "(array([0.72871421, 2.07434434]), 0.031951677737434604)\n",
      "(array([0.73090524, 2.0737439 ]), 0.031437649549528976)\n",
      "(array([0.73307857, 2.07314831]), 0.03093189087974216)\n",
      "(array([0.73523435, 2.07255753]), 0.03043426869075852)\n",
      "(array([0.73737272, 2.07197152]), 0.029944652085524454)\n",
      "(array([0.73949381, 2.07139025]), 0.02946291227281517)\n",
      "(array([0.74159778, 2.07081367]), 0.028988922533357355)\n",
      "(array([0.74368475, 2.07024175]), 0.028522558186495774)\n",
      "(array([0.74575487, 2.06967444]), 0.02806369655739726)\n",
      "(array([0.74780827, 2.06911172]), 0.027612216944781384)\n",
      "(array([0.74984508, 2.06855354]), 0.027168000589170793)\n",
      "(array([0.75186545, 2.06799987]), 0.026730930641651034)\n",
      "(array([0.75386949, 2.06745068]), 0.026300892133136335)\n",
      "(array([0.75585736, 2.06690591]), 0.025877771944125067)\n",
      "(array([0.75782916, 2.06636555]), 0.025461458774945826)\n",
      "(array([0.75978504, 2.06582955]), 0.02505184311647978)\n",
      "(array([0.76172513, 2.06529788]), 0.024648817221355503)\n",
      "(array([0.76364954, 2.06477051]), 0.02425227507560593)\n",
      "(array([0.76555842, 2.06424739]), 0.02386211237078213)\n",
      "(array([0.76745187, 2.0637285 ]), 0.023478226476515752)\n",
      "(array([0.76933004, 2.0632138 ]), 0.02310051641352218)\n",
      "(array([0.77119303, 2.06270326]), 0.022728882827039068)\n",
      "(array([0.77304098, 2.06219684]), 0.02236322796069035)\n",
      "(array([0.774874  , 2.06169451]), 0.022003455630773633)\n",
      "(array([0.77669222, 2.06119624]), 0.021649471200957806)\n",
      "(array([0.77849576, 2.06070199]), 0.021301181557390844)\n",
      "(array([0.78028472, 2.06021173]), 0.020958495084205827)\n",
      "(array([0.78205924, 2.05972543]), 0.020621321639421902)\n",
      "(array([0.78381943, 2.05924306]), 0.020289572531233282)\n",
      "(array([0.7855654 , 2.05876459]), 0.019963160494678896)\n",
      "(array([0.78729728, 2.05828998]), 0.019641999668688327)\n",
      "(array([0.78901516, 2.0578192 ]), 0.019326005573495418)\n",
      "(array([0.79071917, 2.05735223]), 0.019015095088417696)\n",
      "(array([0.79240941, 2.05688903]), 0.018709186429990896)\n",
      "(array([0.79408601, 2.05642957]), 0.01840819913045627)\n",
      "(array([0.79574906, 2.05597382]), 0.018112054016594506)\n",
      "(array([0.79739869, 2.05552175]), 0.017820673188898864)\n",
      "(array([0.79903499, 2.05507333]), 0.017533980001085164)\n",
      "(array([0.80065807, 2.05462853]), 0.01725189903992901)\n",
      "(array([0.80226805, 2.05418733]), 0.01697435610542992)\n",
      "(array([0.80386502, 2.05374968]), 0.016701278191292288)\n",
      "(array([0.80544909, 2.05331558]), 0.016432593465722664)\n",
      "(array([0.80702037, 2.05288498]), 0.01616823125253359)\n",
      "(array([0.80857896, 2.05245785]), 0.015908122012553483)\n",
      "(array([0.81012497, 2.05203418]), 0.01565219732533402)\n",
      "(array([0.81165848, 2.05161393]), 0.015400389871152885)\n",
      "(array([0.81317962, 2.05119707]), 0.015152633413305715)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.81317962, 2.05119707]),\n",
       " <function __main__.cost(hypothesis, training_y)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum(training_x, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our result with the correct $\\theta_0 = 1$ and $\\theta_1 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x219fab221f0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_x, training_y)\n",
    "plt.plot(training_x, hypothesis_theta(training_x, np.array([0.8131, 2.0512])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
